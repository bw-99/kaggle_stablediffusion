{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filepath</th>\n",
       "      <th>prompt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2217ccbd-a1c6-47ac-9a2d-79649727c834.png</td>\n",
       "      <td>a portrait of a female robot made from code, v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>48eb7e17-a3cf-4eb8-96a9-d8e3e23fa1af.png</td>\n",
       "      <td>dream swimming pool with nobody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2919b048-6f68-4ac7-a6d5-060d827abb77.png</td>\n",
       "      <td>a beautiful paint of cultists dancing surround...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3c835fdc-9047-4298-ac8a-7461f5490132.png</td>\n",
       "      <td>frontal portrait of ragged, worried twin women...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>454d7550-a6cf-4896-befb-e2449b281265.png</td>\n",
       "      <td>a stunning portrait of an asian samurai with l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   filepath  \\\n",
       "0  2217ccbd-a1c6-47ac-9a2d-79649727c834.png   \n",
       "1  48eb7e17-a3cf-4eb8-96a9-d8e3e23fa1af.png   \n",
       "2  2919b048-6f68-4ac7-a6d5-060d827abb77.png   \n",
       "3  3c835fdc-9047-4298-ac8a-7461f5490132.png   \n",
       "4  454d7550-a6cf-4896-befb-e2449b281265.png   \n",
       "\n",
       "                                              prompt  \n",
       "0  a portrait of a female robot made from code, v...  \n",
       "1                    dream swimming pool with nobody  \n",
       "2  a beautiful paint of cultists dancing surround...  \n",
       "3  frontal portrait of ragged, worried twin women...  \n",
       "4  a stunning portrait of an asian samurai with l...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import zipfile\n",
    "import torch\n",
    "import io\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "meta_data = pd.read_csv(\"diffusiondb.csv\")\n",
    "for index, filepath in enumerate(meta_data['filepath']):\n",
    "    meta_data.loc[index, 'filepath'] = filepath.split(\"/\")[-1]\n",
    "\n",
    "# print(meta_data['prompt'][0])\n",
    "# print(meta_data['filepath'][0])\n",
    "meta_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "torch.Size([102, 512, 512, 3]) 102\n",
      "tensor([[[ 57.,  44.,  20.],\n",
      "         [ 68.,  61.,  25.],\n",
      "         [ 62.,  51.,  25.],\n",
      "         ...,\n",
      "         [164., 116.,  16.],\n",
      "         [180., 125.,  28.],\n",
      "         [157., 101.,  33.]],\n",
      "\n",
      "        [[ 43.,  32.,  10.],\n",
      "         [ 39.,  25.,   6.],\n",
      "         [ 36.,  24.,   7.],\n",
      "         ...,\n",
      "         [181., 143.,  19.],\n",
      "         [199., 158.,  22.],\n",
      "         [171., 109.,  21.]],\n",
      "\n",
      "        [[ 38.,  29.,   7.],\n",
      "         [ 56.,  40.,  11.],\n",
      "         [ 59.,  48.,  15.],\n",
      "         ...,\n",
      "         [179., 145.,  14.],\n",
      "         [177., 135.,  11.],\n",
      "         [162., 106.,  13.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[104.,  95., 100.],\n",
      "         [133., 122., 126.],\n",
      "         [144., 135., 138.],\n",
      "         ...,\n",
      "         [161., 161., 174.],\n",
      "         [164., 161., 171.],\n",
      "         [158., 150., 156.]],\n",
      "\n",
      "        [[132., 126., 121.],\n",
      "         [135., 126., 128.],\n",
      "         [135., 127., 136.],\n",
      "         ...,\n",
      "         [167., 165., 184.],\n",
      "         [161., 161., 175.],\n",
      "         [150., 147., 152.]],\n",
      "\n",
      "        [[128., 114., 114.],\n",
      "         [128., 121., 122.],\n",
      "         [142., 136., 142.],\n",
      "         ...,\n",
      "         [145., 144., 158.],\n",
      "         [136., 131., 145.],\n",
      "         [165., 158., 162.]]])\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, dataset_dict\n",
    "import zipfile\n",
    "import torch\n",
    "import io\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "zip_data_path =\"diffusiondb-filtered-0-40000.zip\"\n",
    "X = torch.tensor([[[[]]]], dtype=torch.int8)\n",
    "X = torch.zeros(size=(1, 512,512,3))\n",
    "Y = []\n",
    "with zipfile.ZipFile(zip_data_path, \"r\") as zip_data:\n",
    "    content_list = zip_data.namelist()\n",
    "    for index, name_file in enumerate(content_list):\n",
    "        \n",
    "        img_bytes = zip_data.open(name_file)          \n",
    "        img_data = Image.open(img_bytes)              \n",
    "        image_as_array = np.array(img_data, np.uint8)\n",
    "        image_as_array = torch.from_numpy(image_as_array)\n",
    "        image_as_array.unsqueeze_(dim=0)\n",
    "        # print(image_as_array)\n",
    "        # print(image_as_array.shape)\n",
    "        print(meta_data[meta_data.eq(name_file.split(\"/\")[-1]).any(1)][\"prompt\"].item() is str)\n",
    "        Y.append(meta_data[meta_data.eq(name_file.split(\"/\")[-1]).any(1)][\"prompt\"].item())\n",
    "        X = torch.cat([X, image_as_array], dim=0)\n",
    "        # X = np.append(X, image_as_array)\n",
    "        if index == 100:\n",
    "            break\n",
    "# X: 이미지 데이터 배열, Y: 프롬프트 데이터 배열\n",
    "print(X.shape, len(X))\n",
    "X = X[1:]\n",
    "print(X[0])\n",
    "# X = [Image.open(archive.read(file.filename)) for file in file_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "A Singaporean propaganda poster featuring Lee Kuan Yew designed by Saul Bass\n",
      "torch.Size([101, 512, 512, 3])\n"
     ]
    }
   ],
   "source": [
    "print(type(Y[1]))\n",
    "print(Y[1])\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ImageCaptioningDataset(Dataset):\n",
    "    def __init__(self, dataset, processor):\n",
    "        self.dataset = dataset\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "\n",
    "        encoding = self.processor(images=item[\"image\"], text=item[\"text\"], padding=\"max_length\", return_tensors=\"pt\")\n",
    "\n",
    "        # remove batch dimension\n",
    "        encoding = {k:v.squeeze() for k,v in encoding.items()}\n",
    "\n",
    "        return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import AutoProcessor, GitVisionModel\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"microsoft/git-base\")\n",
    "\n",
    "\n",
    "train_data = []\n",
    "for i in range(len(X)):\n",
    "    train_data.append({\n",
    "        \"image\": X[i],\n",
    "        \"text\":Y[i]\n",
    "    })\n",
    "\n",
    "train_dataset = ImageCaptioningDataset(train_data, processor)\n",
    "item = train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids torch.Size([512])\n",
      "attention_mask torch.Size([512])\n",
      "pixel_values torch.Size([3, 224, 224])\n",
      "input_ids torch.Size([2, 512])\n",
      "attention_mask torch.Size([2, 512])\n",
      "pixel_values torch.Size([2, 3, 224, 224])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[CLS] nicolas cage with a bird for hair [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item = train_dataset[0]\n",
    "for k,v in item.items():\n",
    "  print(k,v.shape)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=2)\n",
    "batch = next(iter(train_dataloader))\n",
    "for k,v in batch.items():\n",
    "  print(k,v.shape)\n",
    "  \n",
    "processor.decode(batch[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/git-base\")\n",
    "\n",
    "outputs = model(input_ids=batch[\"input_ids\"],\n",
    "                attention_mask=batch[\"attention_mask\"],\n",
    "                pixel_values=batch[\"pixel_values\"],\n",
    "                labels=batch[\"input_ids\"])\n",
    "outputs.loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bw_anvy_py37",
   "language": "python",
   "name": "bw"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
