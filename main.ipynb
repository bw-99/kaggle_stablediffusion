{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filepath</th>\n",
       "      <th>prompt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2217ccbd-a1c6-47ac-9a2d-79649727c834.png</td>\n",
       "      <td>a portrait of a female robot made from code, v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>48eb7e17-a3cf-4eb8-96a9-d8e3e23fa1af.png</td>\n",
       "      <td>dream swimming pool with nobody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2919b048-6f68-4ac7-a6d5-060d827abb77.png</td>\n",
       "      <td>a beautiful paint of cultists dancing surround...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3c835fdc-9047-4298-ac8a-7461f5490132.png</td>\n",
       "      <td>frontal portrait of ragged, worried twin women...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>454d7550-a6cf-4896-befb-e2449b281265.png</td>\n",
       "      <td>a stunning portrait of an asian samurai with l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   filepath  \\\n",
       "0  2217ccbd-a1c6-47ac-9a2d-79649727c834.png   \n",
       "1  48eb7e17-a3cf-4eb8-96a9-d8e3e23fa1af.png   \n",
       "2  2919b048-6f68-4ac7-a6d5-060d827abb77.png   \n",
       "3  3c835fdc-9047-4298-ac8a-7461f5490132.png   \n",
       "4  454d7550-a6cf-4896-befb-e2449b281265.png   \n",
       "\n",
       "                                              prompt  \n",
       "0  a portrait of a female robot made from code, v...  \n",
       "1                    dream swimming pool with nobody  \n",
       "2  a beautiful paint of cultists dancing surround...  \n",
       "3  frontal portrait of ragged, worried twin women...  \n",
       "4  a stunning portrait of an asian samurai with l...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import zipfile\n",
    "import torch\n",
    "import io\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "meta_data = pd.read_csv(\"diffusiondb.csv\")\n",
    "for index, filepath in enumerate(meta_data['filepath']):\n",
    "    meta_data.loc[index, 'filepath'] = filepath.split(\"/\")[-1]\n",
    "\n",
    "# print(meta_data['prompt'][0])\n",
    "# print(meta_data['filepath'][0])\n",
    "meta_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "torch.Size([102, 512, 512, 3]) 102\n",
      "tensor([[[ 57.,  44.,  20.],\n",
      "         [ 68.,  61.,  25.],\n",
      "         [ 62.,  51.,  25.],\n",
      "         ...,\n",
      "         [164., 116.,  16.],\n",
      "         [180., 125.,  28.],\n",
      "         [157., 101.,  33.]],\n",
      "\n",
      "        [[ 43.,  32.,  10.],\n",
      "         [ 39.,  25.,   6.],\n",
      "         [ 36.,  24.,   7.],\n",
      "         ...,\n",
      "         [181., 143.,  19.],\n",
      "         [199., 158.,  22.],\n",
      "         [171., 109.,  21.]],\n",
      "\n",
      "        [[ 38.,  29.,   7.],\n",
      "         [ 56.,  40.,  11.],\n",
      "         [ 59.,  48.,  15.],\n",
      "         ...,\n",
      "         [179., 145.,  14.],\n",
      "         [177., 135.,  11.],\n",
      "         [162., 106.,  13.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[104.,  95., 100.],\n",
      "         [133., 122., 126.],\n",
      "         [144., 135., 138.],\n",
      "         ...,\n",
      "         [161., 161., 174.],\n",
      "         [164., 161., 171.],\n",
      "         [158., 150., 156.]],\n",
      "\n",
      "        [[132., 126., 121.],\n",
      "         [135., 126., 128.],\n",
      "         [135., 127., 136.],\n",
      "         ...,\n",
      "         [167., 165., 184.],\n",
      "         [161., 161., 175.],\n",
      "         [150., 147., 152.]],\n",
      "\n",
      "        [[128., 114., 114.],\n",
      "         [128., 121., 122.],\n",
      "         [142., 136., 142.],\n",
      "         ...,\n",
      "         [145., 144., 158.],\n",
      "         [136., 131., 145.],\n",
      "         [165., 158., 162.]]])\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, dataset_dict\n",
    "import zipfile\n",
    "import torch\n",
    "import io\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "zip_data_path =\"diffusiondb-filtered-0-40000.zip\"\n",
    "X = torch.tensor([[[[]]]], dtype=torch.int8)\n",
    "X = torch.zeros(size=(1, 512,512,3))\n",
    "Y = []\n",
    "with zipfile.ZipFile(zip_data_path, \"r\") as zip_data:\n",
    "    content_list = zip_data.namelist()\n",
    "    for index, name_file in enumerate(content_list):\n",
    "        \n",
    "        img_bytes = zip_data.open(name_file)          \n",
    "        img_data = Image.open(img_bytes)              \n",
    "        image_as_array = np.array(img_data, np.uint8)\n",
    "        image_as_array = torch.from_numpy(image_as_array)\n",
    "        image_as_array.unsqueeze_(dim=0)\n",
    "        # print(image_as_array)\n",
    "        # print(image_as_array.shape)\n",
    "        print(meta_data[meta_data.eq(name_file.split(\"/\")[-1]).any(1)][\"prompt\"].item() is str)\n",
    "        Y.append(meta_data[meta_data.eq(name_file.split(\"/\")[-1]).any(1)][\"prompt\"].item())\n",
    "        X = torch.cat([X, image_as_array], dim=0)\n",
    "        # X = np.append(X, image_as_array)\n",
    "        # if index == 100:\n",
    "        #     break\n",
    "# X: 이미지 데이터 배열, Y: 프롬프트 데이터 배열\n",
    "print(X.shape, len(X))\n",
    "X = X[1:]\n",
    "print(X[0])\n",
    "# X = [Image.open(archive.read(file.filename)) for file in file_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "A Singaporean propaganda poster featuring Lee Kuan Yew designed by Saul Bass\n",
      "torch.Size([101, 512, 512, 3])\n"
     ]
    }
   ],
   "source": [
    "print(type(Y[1]))\n",
    "print(Y[1])\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ImageCaptioningDataset(Dataset):\n",
    "    def __init__(self, dataset, processor):\n",
    "        self.dataset = dataset\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "\n",
    "        encoding = self.processor(images=item[\"image\"], text=item[\"text\"], padding=\"max_length\", return_tensors=\"pt\")\n",
    "\n",
    "        # remove batch dimension\n",
    "        encoding = {k:v.squeeze() for k,v in encoding.items()}\n",
    "\n",
    "        return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import AutoProcessor, GitVisionModel\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"microsoft/git-base\")\n",
    "\n",
    "\n",
    "train_data = []\n",
    "for i in range(len(X)):\n",
    "    train_data.append({\n",
    "        \"image\": X[i],\n",
    "        \"text\":Y[i]\n",
    "    })\n",
    "\n",
    "train_dataset = ImageCaptioningDataset(train_data, processor)\n",
    "item = train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids torch.Size([512])\n",
      "attention_mask torch.Size([512])\n",
      "pixel_values torch.Size([3, 224, 224])\n",
      "input_ids torch.Size([2, 512])\n",
      "attention_mask torch.Size([2, 512])\n",
      "pixel_values torch.Size([2, 3, 224, 224])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[CLS] nicolas cage with a bird for hair [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item = train_dataset[0]\n",
    "for k,v in item.items():\n",
    "  print(k,v.shape)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=2)\n",
    "batch = next(iter(train_dataloader))\n",
    "for k,v in batch.items():\n",
    "  print(k,v.shape)\n",
    "  \n",
    "processor.decode(batch[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20343e69c63c4ba88a56340f98c113a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/2.82k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b91c91fec80c450e8ef2ef520bd0e159",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/707M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bc51dd1b1b64fc593c50e11aa4b4a1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/141 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor(12.0935, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/git-base\")\n",
    "\n",
    "outputs = model(input_ids=batch[\"input_ids\"],\n",
    "                attention_mask=batch[\"attention_mask\"],\n",
    "                pixel_values=batch[\"pixel_values\"],\n",
    "                labels=batch[\"input_ids\"])\n",
    "outputs.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Loss: 12.433830261230469\n",
      "Loss: 10.63500690460205\n",
      "Loss: 10.167282104492188\n",
      "Loss: 9.888833045959473\n",
      "Loss: 9.522363662719727\n",
      "Loss: 9.440932273864746\n",
      "Loss: 9.282150268554688\n",
      "Loss: 9.082950592041016\n",
      "Loss: 8.910561561584473\n",
      "Loss: 8.812605857849121\n",
      "Loss: 8.649910926818848\n",
      "Loss: 8.599287033081055\n",
      "Loss: 8.346343994140625\n",
      "Loss: 8.327411651611328\n",
      "Loss: 8.191781044006348\n",
      "Loss: 8.095382690429688\n",
      "Loss: 7.9943318367004395\n",
      "Loss: 7.829907417297363\n",
      "Loss: 7.769596099853516\n",
      "Loss: 7.703853607177734\n",
      "Loss: 7.57621955871582\n",
      "Loss: 7.488241195678711\n",
      "Loss: 7.400453090667725\n",
      "Loss: 7.297728538513184\n",
      "Loss: 7.072103500366211\n",
      "Loss: 7.0852131843566895\n",
      "Loss: 6.979156494140625\n",
      "Loss: 6.853201866149902\n",
      "Loss: 6.808433532714844\n",
      "Loss: 6.6874589920043945\n",
      "Loss: 6.580168724060059\n",
      "Loss: 6.469264507293701\n",
      "Loss: 6.393990993499756\n",
      "Loss: 6.314325332641602\n",
      "Loss: 6.179984092712402\n",
      "Loss: 6.096042156219482\n",
      "Loss: 5.978611469268799\n",
      "Loss: 5.843764305114746\n",
      "Loss: 5.730334758758545\n",
      "Loss: 5.631422519683838\n",
      "Loss: 5.484210014343262\n",
      "Loss: 5.424258708953857\n",
      "Loss: 5.428139686584473\n",
      "Loss: 5.169261932373047\n",
      "Loss: 5.081932067871094\n",
      "Loss: 4.9734015464782715\n",
      "Loss: 4.8900275230407715\n",
      "Loss: 4.891932964324951\n",
      "Loss: 4.68702507019043\n",
      "Loss: 4.540047645568848\n",
      "Loss: 4.5611491203308105\n",
      "Epoch: 1\n",
      "Loss: 4.28141450881958\n",
      "Loss: 4.205246925354004\n",
      "Loss: 4.106906414031982\n",
      "Loss: 4.000870227813721\n",
      "Loss: 3.881486177444458\n",
      "Loss: 3.772325277328491\n",
      "Loss: 3.6626431941986084\n",
      "Loss: 3.5273797512054443\n",
      "Loss: 3.6704540252685547\n",
      "Loss: 3.3799266815185547\n",
      "Loss: 3.29968523979187\n",
      "Loss: 3.097180128097534\n",
      "Loss: 3.0071146488189697\n",
      "Loss: 2.840123414993286\n",
      "Loss: 2.776116132736206\n",
      "Loss: 2.6832122802734375\n",
      "Loss: 2.6724393367767334\n",
      "Loss: 2.7049667835235596\n",
      "Loss: 2.3666133880615234\n",
      "Loss: 2.2926642894744873\n",
      "Loss: 2.1574478149414062\n",
      "Loss: 2.0939931869506836\n",
      "Loss: 2.03147554397583\n",
      "Loss: 1.9721226692199707\n",
      "Loss: 1.8425065279006958\n",
      "Loss: 1.6945693492889404\n",
      "Loss: 1.5877553224563599\n",
      "Loss: 1.544402003288269\n",
      "Loss: 1.521124243736267\n",
      "Loss: 1.3712944984436035\n",
      "Loss: 1.5266433954238892\n",
      "Loss: 1.2107728719711304\n",
      "Loss: 1.1278927326202393\n",
      "Loss: 1.1717149019241333\n",
      "Loss: 1.0506846904754639\n",
      "Loss: 0.9576156735420227\n",
      "Loss: 0.9551299810409546\n",
      "Loss: 0.8343935012817383\n",
      "Loss: 0.8298171162605286\n",
      "Loss: 0.8684913516044617\n",
      "Loss: 0.7585151791572571\n",
      "Loss: 0.8925241827964783\n",
      "Loss: 0.6651954054832458\n",
      "Loss: 0.7766433954238892\n",
      "Loss: 0.5748302340507507\n",
      "Loss: 0.7603247165679932\n",
      "Loss: 0.5656029582023621\n",
      "Loss: 0.4906310737133026\n",
      "Loss: 0.5316412448883057\n",
      "Loss: 0.557712972164154\n",
      "Loss: 0.46573057770729065\n",
      "Epoch: 2\n",
      "Loss: 0.40239593386650085\n",
      "Loss: 0.5696046352386475\n",
      "Loss: 0.4744117856025696\n",
      "Loss: 0.3691171407699585\n",
      "Loss: 0.44110891222953796\n",
      "Loss: 0.3084404170513153\n",
      "Loss: 0.36599916219711304\n",
      "Loss: 0.3435010313987732\n",
      "Loss: 0.3237484395503998\n",
      "Loss: 0.4375584125518799\n",
      "Loss: 0.27914395928382874\n",
      "Loss: 0.3403751850128174\n",
      "Loss: 0.347031831741333\n",
      "Loss: 0.4028557240962982\n",
      "Loss: 0.36216259002685547\n",
      "Loss: 0.32843339443206787\n",
      "Loss: 0.24229176342487335\n",
      "Loss: 0.23089636862277985\n",
      "Loss: 0.25493380427360535\n",
      "Loss: 0.24605309963226318\n",
      "Loss: 0.3169373869895935\n",
      "Loss: 0.2352396696805954\n",
      "Loss: 0.3268926739692688\n",
      "Loss: 0.2827785909175873\n",
      "Loss: 0.23804888129234314\n",
      "Loss: 0.40545645356178284\n",
      "Loss: 0.2219255566596985\n",
      "Loss: 0.4465820789337158\n",
      "Loss: 0.27273768186569214\n",
      "Loss: 0.3036019504070282\n",
      "Loss: 0.18752577900886536\n",
      "Loss: 0.377708375453949\n",
      "Loss: 0.20968659222126007\n",
      "Loss: 0.2998548746109009\n",
      "Loss: 0.19969147443771362\n",
      "Loss: 0.2530910074710846\n",
      "Loss: 0.23517875373363495\n",
      "Loss: 0.2479286640882492\n",
      "Loss: 0.14839984476566315\n",
      "Loss: 0.26314228773117065\n",
      "Loss: 0.2879047691822052\n",
      "Loss: 0.21058300137519836\n",
      "Loss: 0.28975486755371094\n",
      "Loss: 0.19191794097423553\n",
      "Loss: 0.21932262182235718\n",
      "Loss: 0.328372597694397\n",
      "Loss: 0.1890726387500763\n",
      "Loss: 0.17739306390285492\n",
      "Loss: 0.24877771735191345\n",
      "Loss: 0.3986782431602478\n",
      "Loss: 0.15818387269973755\n",
      "Epoch: 3\n",
      "Loss: 0.16426126658916473\n",
      "Loss: 0.1427859514951706\n",
      "Loss: 0.20674505829811096\n",
      "Loss: 0.12333597242832184\n",
      "Loss: 0.19826991856098175\n",
      "Loss: 0.2429353892803192\n",
      "Loss: 0.1394575834274292\n",
      "Loss: 0.1342368870973587\n",
      "Loss: 0.17445175349712372\n",
      "Loss: 0.16347835958003998\n",
      "Loss: 0.12200020998716354\n",
      "Loss: 0.19313877820968628\n",
      "Loss: 0.16034576296806335\n",
      "Loss: 0.18324518203735352\n",
      "Loss: 0.14302732050418854\n",
      "Loss: 0.22638417780399323\n",
      "Loss: 0.09784549474716187\n",
      "Loss: 0.18747609853744507\n",
      "Loss: 0.1661454439163208\n",
      "Loss: 0.14862023293972015\n",
      "Loss: 0.2865278422832489\n",
      "Loss: 0.24954095482826233\n",
      "Loss: 0.11820058524608612\n",
      "Loss: 0.19903802871704102\n",
      "Loss: 0.21682439744472504\n",
      "Loss: 0.21579550206661224\n",
      "Loss: 0.11868924647569656\n",
      "Loss: 0.16179931163787842\n",
      "Loss: 0.1463060975074768\n",
      "Loss: 0.11176713556051254\n",
      "Loss: 0.13447125256061554\n",
      "Loss: 0.1483687162399292\n",
      "Loss: 0.22571906447410583\n",
      "Loss: 0.23387965559959412\n",
      "Loss: 0.09598509222269058\n",
      "Loss: 0.09418666362762451\n",
      "Loss: 0.10447264462709427\n",
      "Loss: 0.32805129885673523\n",
      "Loss: 0.16468623280525208\n",
      "Loss: 0.1500851809978485\n",
      "Loss: 0.12893499433994293\n",
      "Loss: 0.14850904047489166\n",
      "Loss: 0.1860024780035019\n",
      "Loss: 0.16367127001285553\n",
      "Loss: 0.22108036279678345\n",
      "Loss: 0.25533246994018555\n",
      "Loss: 0.2379101663827896\n",
      "Loss: 0.23757678270339966\n",
      "Loss: 0.29806259274482727\n",
      "Loss: 0.09643497318029404\n",
      "Loss: 0.08286220580339432\n",
      "Epoch: 4\n",
      "Loss: 0.09998462349176407\n",
      "Loss: 0.14229995012283325\n",
      "Loss: 0.12800854444503784\n",
      "Loss: 0.11336056143045425\n",
      "Loss: 0.09709416329860687\n",
      "Loss: 0.16764138638973236\n",
      "Loss: 0.22828252613544464\n",
      "Loss: 0.12559832632541656\n",
      "Loss: 0.11380132287740707\n",
      "Loss: 0.14952702820301056\n",
      "Loss: 0.09288103133440018\n",
      "Loss: 0.1268555223941803\n",
      "Loss: 0.19718797504901886\n",
      "Loss: 0.07616500556468964\n",
      "Loss: 0.1905219703912735\n",
      "Loss: 0.09727780520915985\n",
      "Loss: 0.07611051946878433\n",
      "Loss: 0.24982137978076935\n",
      "Loss: 0.1015101745724678\n",
      "Loss: 0.16206301748752594\n",
      "Loss: 0.14609724283218384\n",
      "Loss: 0.14121264219284058\n",
      "Loss: 0.13185659050941467\n",
      "Loss: 0.09727126359939575\n",
      "Loss: 0.1034289300441742\n",
      "Loss: 0.08417269587516785\n",
      "Loss: 0.09356538206338882\n",
      "Loss: 0.11850233376026154\n",
      "Loss: 0.21438279747962952\n",
      "Loss: 0.0712326243519783\n",
      "Loss: 0.14011825621128082\n",
      "Loss: 0.11384508013725281\n",
      "Loss: 0.24012258648872375\n",
      "Loss: 0.11060957610607147\n",
      "Loss: 0.07042433321475983\n",
      "Loss: 0.11623566597700119\n",
      "Loss: 0.14892293512821198\n",
      "Loss: 0.12800303101539612\n",
      "Loss: 0.08563432097434998\n",
      "Loss: 0.17061591148376465\n",
      "Loss: 0.12561634182929993\n",
      "Loss: 0.0700211301445961\n",
      "Loss: 0.07475007325410843\n",
      "Loss: 0.19847412407398224\n",
      "Loss: 0.058820366859436035\n",
      "Loss: 0.12174705415964127\n",
      "Loss: 0.13587889075279236\n",
      "Loss: 0.10256031900644302\n",
      "Loss: 0.14706960320472717\n",
      "Loss: 0.06182195991277695\n",
      "Loss: 0.06525672972202301\n",
      "Epoch: 5\n",
      "Loss: 0.07647129148244858\n",
      "Loss: 0.08623402565717697\n",
      "Loss: 0.08351726084947586\n",
      "Loss: 0.10050255805253983\n",
      "Loss: 0.0481652207672596\n",
      "Loss: 0.13754835724830627\n",
      "Loss: 0.062472760677337646\n",
      "Loss: 0.05627479404211044\n",
      "Loss: 0.12184489518404007\n",
      "Loss: 0.11446850746870041\n",
      "Loss: 0.053195126354694366\n",
      "Loss: 0.05735941231250763\n",
      "Loss: 0.103800930082798\n",
      "Loss: 0.13966894149780273\n",
      "Loss: 0.07880593091249466\n",
      "Loss: 0.08632025867700577\n",
      "Loss: 0.16536377370357513\n",
      "Loss: 0.09600257128477097\n",
      "Loss: 0.10524882376194\n",
      "Loss: 0.0705593153834343\n",
      "Loss: 0.11575460433959961\n",
      "Loss: 0.11707225441932678\n",
      "Loss: 0.08147748559713364\n",
      "Loss: 0.10441596806049347\n",
      "Loss: 0.06553210318088531\n",
      "Loss: 0.07935099303722382\n",
      "Loss: 0.05076795816421509\n",
      "Loss: 0.06228959187865257\n",
      "Loss: 0.06640350818634033\n",
      "Loss: 0.05941179394721985\n",
      "Loss: 0.1275331676006317\n",
      "Loss: 0.07006741315126419\n",
      "Loss: 0.15060517191886902\n",
      "Loss: 0.1253947764635086\n",
      "Loss: 0.06598380208015442\n",
      "Loss: 0.18843671679496765\n",
      "Loss: 0.05755937099456787\n",
      "Loss: 0.05055094137787819\n",
      "Loss: 0.08427448570728302\n",
      "Loss: 0.06849920749664307\n",
      "Loss: 0.07525187730789185\n",
      "Loss: 0.06291133910417557\n",
      "Loss: 0.12009400874376297\n",
      "Loss: 0.07261166721582413\n",
      "Loss: 0.09394367039203644\n",
      "Loss: 0.07817090302705765\n",
      "Loss: 0.1446526050567627\n",
      "Loss: 0.06379757821559906\n",
      "Loss: 0.12013057619333267\n",
      "Loss: 0.1369350254535675\n",
      "Loss: 0.17489151656627655\n",
      "Epoch: 6\n",
      "Loss: 0.042087655514478683\n",
      "Loss: 0.04987124353647232\n",
      "Loss: 0.05970725417137146\n",
      "Loss: 0.05534234270453453\n",
      "Loss: 0.07561253011226654\n",
      "Loss: 0.09771610796451569\n",
      "Loss: 0.07194880396127701\n",
      "Loss: 0.043720223009586334\n",
      "Loss: 0.067467600107193\n",
      "Loss: 0.10241515934467316\n",
      "Loss: 0.07114379853010178\n",
      "Loss: 0.04736160859465599\n",
      "Loss: 0.048208385705947876\n",
      "Loss: 0.06612620502710342\n",
      "Loss: 0.16349081695079803\n",
      "Loss: 0.08876627683639526\n",
      "Loss: 0.04003984481096268\n",
      "Loss: 0.10301777720451355\n",
      "Loss: 0.04668644815683365\n",
      "Loss: 0.053256917744874954\n",
      "Loss: 0.09788000583648682\n",
      "Loss: 0.055675093084573746\n",
      "Loss: 0.055756162852048874\n",
      "Loss: 0.062497809529304504\n",
      "Loss: 0.059457141906023026\n",
      "Loss: 0.060846660286188126\n",
      "Loss: 0.06099963188171387\n",
      "Loss: 0.07243143022060394\n",
      "Loss: 0.13052643835544586\n",
      "Loss: 0.041072942316532135\n",
      "Loss: 0.04810292273759842\n",
      "Loss: 0.054817184805870056\n",
      "Loss: 0.08098946511745453\n",
      "Loss: 0.12810678780078888\n",
      "Loss: 0.051581840962171555\n",
      "Loss: 0.03829696401953697\n",
      "Loss: 0.07797690480947495\n",
      "Loss: 0.05580298975110054\n",
      "Loss: 0.11842082440853119\n",
      "Loss: 0.05329445004463196\n",
      "Loss: 0.09732771664857864\n",
      "Loss: 0.0589241199195385\n",
      "Loss: 0.04984574764966965\n",
      "Loss: 0.06652719527482986\n",
      "Loss: 0.06107523292303085\n",
      "Loss: 0.051108017563819885\n",
      "Loss: 0.11336769908666611\n",
      "Loss: 0.07493934780359268\n",
      "Loss: 0.06252922862768173\n",
      "Loss: 0.08856286108493805\n",
      "Loss: 0.05075782909989357\n",
      "Epoch: 7\n",
      "Loss: 0.0458548367023468\n",
      "Loss: 0.07683125883340836\n",
      "Loss: 0.08749174326658249\n",
      "Loss: 0.04913853481411934\n",
      "Loss: 0.06908932328224182\n",
      "Loss: 0.04251326620578766\n",
      "Loss: 0.052119046449661255\n",
      "Loss: 0.0405375100672245\n",
      "Loss: 0.04603567346930504\n",
      "Loss: 0.03667694330215454\n",
      "Loss: 0.038939278572797775\n",
      "Loss: 0.06685389578342438\n",
      "Loss: 0.0350424088537693\n",
      "Loss: 0.045655857771635056\n",
      "Loss: 0.050776880234479904\n",
      "Loss: 0.07553059607744217\n",
      "Loss: 0.03749122843146324\n",
      "Loss: 0.09907405078411102\n",
      "Loss: 0.060696810483932495\n",
      "Loss: 0.04882218316197395\n",
      "Loss: 0.040211476385593414\n",
      "Loss: 0.03703765571117401\n",
      "Loss: 0.047054290771484375\n",
      "Loss: 0.0571591854095459\n",
      "Loss: 0.08477667719125748\n",
      "Loss: 0.047113705426454544\n",
      "Loss: 0.043318744748830795\n",
      "Loss: 0.05371714383363724\n",
      "Loss: 0.03259889781475067\n",
      "Loss: 0.13260823488235474\n",
      "Loss: 0.07513236254453659\n",
      "Loss: 0.08107443898916245\n",
      "Loss: 0.08736559748649597\n",
      "Loss: 0.05600132420659065\n",
      "Loss: 0.04248505085706711\n",
      "Loss: 0.035587430000305176\n",
      "Loss: 0.03380873054265976\n",
      "Loss: 0.04507370665669441\n",
      "Loss: 0.04076670855283737\n",
      "Loss: 0.036893296986818314\n",
      "Loss: 0.037739843130111694\n",
      "Loss: 0.04302331432700157\n",
      "Loss: 0.06925380975008011\n",
      "Loss: 0.07952169328927994\n",
      "Loss: 0.06347515434026718\n",
      "Loss: 0.04133964702486992\n",
      "Loss: 0.04873746633529663\n",
      "Loss: 0.0465175025165081\n",
      "Loss: 0.02971707098186016\n",
      "Loss: 0.03568413108587265\n",
      "Loss: 0.08754347264766693\n",
      "Epoch: 8\n",
      "Loss: 0.04870004579424858\n",
      "Loss: 0.05896979942917824\n",
      "Loss: 0.02557576261460781\n",
      "Loss: 0.030896920710802078\n",
      "Loss: 0.040365710854530334\n",
      "Loss: 0.042063914239406586\n",
      "Loss: 0.04702143371105194\n",
      "Loss: 0.07836028188467026\n",
      "Loss: 0.05378599837422371\n",
      "Loss: 0.039980221539735794\n",
      "Loss: 0.03363021835684776\n",
      "Loss: 0.02697243168950081\n",
      "Loss: 0.07798569649457932\n",
      "Loss: 0.0327044352889061\n",
      "Loss: 0.036153148859739304\n",
      "Loss: 0.045532725751399994\n",
      "Loss: 0.0318366140127182\n",
      "Loss: 0.06262426823377609\n",
      "Loss: 0.03882024437189102\n",
      "Loss: 0.08486546576023102\n",
      "Loss: 0.026393070816993713\n",
      "Loss: 0.030148159712553024\n",
      "Loss: 0.0435083843767643\n",
      "Loss: 0.048360425978899\n",
      "Loss: 0.028805144131183624\n",
      "Loss: 0.09531546384096146\n",
      "Loss: 0.04105918109416962\n",
      "Loss: 0.04909497871994972\n",
      "Loss: 0.035450953990221024\n",
      "Loss: 0.073151133954525\n",
      "Loss: 0.02936110459268093\n",
      "Loss: 0.03965487703680992\n",
      "Loss: 0.033022619783878326\n",
      "Loss: 0.048336390405893326\n",
      "Loss: 0.041918087750673294\n",
      "Loss: 0.051792994141578674\n",
      "Loss: 0.04070157930254936\n",
      "Loss: 0.03598787263035774\n",
      "Loss: 0.0347369983792305\n",
      "Loss: 0.06240416318178177\n",
      "Loss: 0.0452476404607296\n",
      "Loss: 0.03473706915974617\n",
      "Loss: 0.03481242060661316\n",
      "Loss: 0.06344034522771835\n",
      "Loss: 0.05868959799408913\n",
      "Loss: 0.040991466492414474\n",
      "Loss: 0.061394102871418\n",
      "Loss: 0.0455179326236248\n",
      "Loss: 0.04192402958869934\n",
      "Loss: 0.03002907522022724\n",
      "Loss: 0.034990061074495316\n",
      "Epoch: 9\n",
      "Loss: 0.03252502903342247\n",
      "Loss: 0.03802506998181343\n",
      "Loss: 0.036371469497680664\n",
      "Loss: 0.038271598517894745\n",
      "Loss: 0.029055370017886162\n",
      "Loss: 0.024337660521268845\n",
      "Loss: 0.060250334441661835\n",
      "Loss: 0.03745061531662941\n",
      "Loss: 0.02636100724339485\n",
      "Loss: 0.04086579382419586\n",
      "Loss: 0.02609303779900074\n",
      "Loss: 0.03074498660862446\n",
      "Loss: 0.030352452769875526\n",
      "Loss: 0.02517392858862877\n",
      "Loss: 0.041559845209121704\n",
      "Loss: 0.05710143595933914\n",
      "Loss: 0.0301518514752388\n",
      "Loss: 0.03960921987891197\n",
      "Loss: 0.03743299841880798\n",
      "Loss: 0.03427645564079285\n",
      "Loss: 0.024172646924853325\n",
      "Loss: 0.0332687571644783\n",
      "Loss: 0.026741964742541313\n",
      "Loss: 0.025776095688343048\n",
      "Loss: 0.055582232773303986\n",
      "Loss: 0.03439885377883911\n",
      "Loss: 0.038938697427511215\n",
      "Loss: 0.024671826511621475\n",
      "Loss: 0.0275291595607996\n",
      "Loss: 0.05131256580352783\n",
      "Loss: 0.02177244983613491\n",
      "Loss: 0.07779887318611145\n",
      "Loss: 0.031221581622958183\n",
      "Loss: 0.056012317538261414\n",
      "Loss: 0.06284593790769577\n",
      "Loss: 0.05152682587504387\n",
      "Loss: 0.04618809372186661\n",
      "Loss: 0.05767834931612015\n",
      "Loss: 0.05322286859154701\n",
      "Loss: 0.03535857051610947\n",
      "Loss: 0.03314699977636337\n",
      "Loss: 0.032303109765052795\n",
      "Loss: 0.03410995006561279\n",
      "Loss: 0.027223816141486168\n",
      "Loss: 0.04964959993958473\n",
      "Loss: 0.04475643113255501\n",
      "Loss: 0.030903950333595276\n",
      "Loss: 0.02416357398033142\n",
      "Loss: 0.03180095925927162\n",
      "Loss: 0.037042275071144104\n",
      "Loss: 0.021220941096544266\n",
      "Epoch: 10\n",
      "Loss: 0.02832770347595215\n",
      "Loss: 0.0223480686545372\n",
      "Loss: 0.03422493487596512\n",
      "Loss: 0.03744984790682793\n",
      "Loss: 0.04722213000059128\n",
      "Loss: 0.023993832990527153\n",
      "Loss: 0.021263033151626587\n",
      "Loss: 0.06396739184856415\n",
      "Loss: 0.037843141704797745\n",
      "Loss: 0.04846017807722092\n",
      "Loss: 0.027921581640839577\n",
      "Loss: 0.02286980301141739\n",
      "Loss: 0.02247178740799427\n",
      "Loss: 0.027032645419239998\n",
      "Loss: 0.03917469456791878\n",
      "Loss: 0.02427518554031849\n",
      "Loss: 0.021793926134705544\n",
      "Loss: 0.033240873366594315\n",
      "Loss: 0.025590473785996437\n",
      "Loss: 0.028643393889069557\n",
      "Loss: 0.06494239717721939\n",
      "Loss: 0.05089312791824341\n",
      "Loss: 0.02728472650051117\n",
      "Loss: 0.023367488756775856\n",
      "Loss: 0.021925242617726326\n",
      "Loss: 0.03731609135866165\n",
      "Loss: 0.038234859704971313\n",
      "Loss: 0.026928842067718506\n",
      "Loss: 0.02536691166460514\n",
      "Loss: 0.02182706817984581\n",
      "Loss: 0.02915281616151333\n",
      "Loss: 0.03840844705700874\n",
      "Loss: 0.06528208404779434\n",
      "Loss: 0.020840825513005257\n",
      "Loss: 0.02244672179222107\n",
      "Loss: 0.0337679497897625\n",
      "Loss: 0.041134536266326904\n",
      "Loss: 0.03627103939652443\n",
      "Loss: 0.024282651022076607\n",
      "Loss: 0.040691182017326355\n",
      "Loss: 0.04526353254914284\n",
      "Loss: 0.04510360583662987\n",
      "Loss: 0.021033838391304016\n",
      "Loss: 0.037666335701942444\n",
      "Loss: 0.026803556829690933\n",
      "Loss: 0.027971791103482246\n",
      "Loss: 0.037308160215616226\n",
      "Loss: 0.05042896047234535\n",
      "Loss: 0.0373016893863678\n",
      "Loss: 0.025233861058950424\n",
      "Loss: 0.03261976316571236\n",
      "Epoch: 11\n",
      "Loss: 0.04809766262769699\n",
      "Loss: 0.025070540606975555\n",
      "Loss: 0.02636074647307396\n",
      "Loss: 0.03480001538991928\n",
      "Loss: 0.029394851997494698\n",
      "Loss: 0.021717464551329613\n",
      "Loss: 0.025461165234446526\n",
      "Loss: 0.021416643634438515\n",
      "Loss: 0.0374397337436676\n",
      "Loss: 0.024121426045894623\n",
      "Loss: 0.027925122529268265\n",
      "Loss: 0.02224123105406761\n",
      "Loss: 0.02150435373187065\n",
      "Loss: 0.01957189477980137\n",
      "Loss: 0.02054472081363201\n",
      "Loss: 0.01751996949315071\n",
      "Loss: 0.04559340700507164\n",
      "Loss: 0.01926765963435173\n",
      "Loss: 0.020560476928949356\n",
      "Loss: 0.01927267573773861\n",
      "Loss: 0.03272038325667381\n",
      "Loss: 0.019992347806692123\n",
      "Loss: 0.018419329077005386\n",
      "Loss: 0.04666859656572342\n",
      "Loss: 0.03622588887810707\n",
      "Loss: 0.03980814293026924\n",
      "Loss: 0.022003235295414925\n",
      "Loss: 0.019771059975028038\n",
      "Loss: 0.07026781141757965\n",
      "Loss: 0.04017737880349159\n",
      "Loss: 0.024220868945121765\n",
      "Loss: 0.028879813849925995\n",
      "Loss: 0.027364633977413177\n",
      "Loss: 0.018123680725693703\n",
      "Loss: 0.03958028182387352\n",
      "Loss: 0.03842001035809517\n",
      "Loss: 0.047085054218769073\n",
      "Loss: 0.021766480058431625\n",
      "Loss: 0.03315229341387749\n",
      "Loss: 0.028377169743180275\n",
      "Loss: 0.0192854106426239\n",
      "Loss: 0.024377604946494102\n",
      "Loss: 0.02971319481730461\n",
      "Loss: 0.03605978563427925\n",
      "Loss: 0.02910134755074978\n",
      "Loss: 0.03730569779872894\n",
      "Loss: 0.020503390580415726\n",
      "Loss: 0.02484557405114174\n",
      "Loss: 0.02130555361509323\n",
      "Loss: 0.0211444403976202\n",
      "Loss: 0.0423881933093071\n",
      "Epoch: 12\n",
      "Loss: 0.021844029426574707\n",
      "Loss: 0.027447210624814034\n",
      "Loss: 0.0160103477537632\n",
      "Loss: 0.04028383642435074\n",
      "Loss: 0.025341706350445747\n",
      "Loss: 0.016877133399248123\n",
      "Loss: 0.0263838954269886\n",
      "Loss: 0.02193434163928032\n",
      "Loss: 0.022700557485222816\n",
      "Loss: 0.04149537906050682\n",
      "Loss: 0.03161994367837906\n",
      "Loss: 0.014348783530294895\n",
      "Loss: 0.0257291030138731\n",
      "Loss: 0.02383112907409668\n",
      "Loss: 0.03102526068687439\n",
      "Loss: 0.04100258648395538\n",
      "Loss: 0.020080972462892532\n",
      "Loss: 0.018059099093079567\n",
      "Loss: 0.020420121029019356\n",
      "Loss: 0.04277674853801727\n",
      "Loss: 0.0167471244931221\n",
      "Loss: 0.018962405622005463\n",
      "Loss: 0.032041050493717194\n",
      "Loss: 0.05009036511182785\n",
      "Loss: 0.020348699763417244\n",
      "Loss: 0.018090562894940376\n",
      "Loss: 0.02672126516699791\n",
      "Loss: 0.018371211364865303\n",
      "Loss: 0.017423124983906746\n",
      "Loss: 0.02461966872215271\n",
      "Loss: 0.02036745473742485\n",
      "Loss: 0.02063770778477192\n",
      "Loss: 0.025408899411559105\n",
      "Loss: 0.020388830453157425\n",
      "Loss: 0.03402547165751457\n",
      "Loss: 0.025464026257395744\n",
      "Loss: 0.05007500201463699\n",
      "Loss: 0.03551088273525238\n",
      "Loss: 0.023498034104704857\n",
      "Loss: 0.022025397047400475\n",
      "Loss: 0.020174572244286537\n",
      "Loss: 0.03099583089351654\n",
      "Loss: 0.022308846935629845\n",
      "Loss: 0.01881732977926731\n",
      "Loss: 0.018935097381472588\n",
      "Loss: 0.02309718355536461\n",
      "Loss: 0.019136304035782814\n",
      "Loss: 0.02895178087055683\n",
      "Loss: 0.020053425803780556\n",
      "Loss: 0.02951459214091301\n",
      "Loss: 0.03185545653104782\n",
      "Epoch: 13\n",
      "Loss: 0.018483933061361313\n",
      "Loss: 0.017012029886245728\n",
      "Loss: 0.01550530269742012\n",
      "Loss: 0.01290324330329895\n",
      "Loss: 0.0341816172003746\n",
      "Loss: 0.017367469146847725\n",
      "Loss: 0.01667974516749382\n",
      "Loss: 0.013899027369916439\n",
      "Loss: 0.02534930780529976\n",
      "Loss: 0.01707007922232151\n",
      "Loss: 0.013996335677802563\n",
      "Loss: 0.037357397377491\n",
      "Loss: 0.020306942984461784\n",
      "Loss: 0.028462199494242668\n",
      "Loss: 0.014617905020713806\n",
      "Loss: 0.03303738310933113\n",
      "Loss: 0.01807238906621933\n",
      "Loss: 0.030244173482060432\n",
      "Loss: 0.052219320088624954\n",
      "Loss: 0.016656337305903435\n",
      "Loss: 0.01578003540635109\n",
      "Loss: 0.024081360548734665\n",
      "Loss: 0.029028914868831635\n",
      "Loss: 0.012473509646952152\n",
      "Loss: 0.03148798272013664\n",
      "Loss: 0.02165146917104721\n",
      "Loss: 0.019328944385051727\n",
      "Loss: 0.01853349804878235\n",
      "Loss: 0.02312547340989113\n",
      "Loss: 0.029724275693297386\n",
      "Loss: 0.018766185268759727\n",
      "Loss: 0.0191641878336668\n",
      "Loss: 0.014352370984852314\n",
      "Loss: 0.017087414860725403\n",
      "Loss: 0.025603219866752625\n",
      "Loss: 0.019091768190264702\n",
      "Loss: 0.025515787303447723\n",
      "Loss: 0.021697629243135452\n",
      "Loss: 0.020551716908812523\n",
      "Loss: 0.028736403211951256\n",
      "Loss: 0.01850012317299843\n",
      "Loss: 0.05028248205780983\n",
      "Loss: 0.021592458710074425\n",
      "Loss: 0.031459301710128784\n",
      "Loss: 0.03189428150653839\n",
      "Loss: 0.015141797251999378\n",
      "Loss: 0.01583939976990223\n",
      "Loss: 0.03319469839334488\n",
      "Loss: 0.022242406383156776\n",
      "Loss: 0.02011208049952984\n",
      "Loss: 0.03183799237012863\n",
      "Epoch: 14\n",
      "Loss: 0.01909594237804413\n",
      "Loss: 0.038535572588443756\n",
      "Loss: 0.014351217076182365\n",
      "Loss: 0.018238605931401253\n",
      "Loss: 0.01692606508731842\n",
      "Loss: 0.016033945605158806\n",
      "Loss: 0.0193083006888628\n",
      "Loss: 0.019772455096244812\n",
      "Loss: 0.013582181185483932\n",
      "Loss: 0.016439083963632584\n",
      "Loss: 0.016363414004445076\n",
      "Loss: 0.015241705812513828\n",
      "Loss: 0.020449155941605568\n",
      "Loss: 0.016584740951657295\n",
      "Loss: 0.012620994821190834\n",
      "Loss: 0.016932344064116478\n",
      "Loss: 0.020589729771018028\n",
      "Loss: 0.020284781232476234\n",
      "Loss: 0.03470190614461899\n",
      "Loss: 0.01766904629766941\n",
      "Loss: 0.023953169584274292\n",
      "Loss: 0.019109878689050674\n",
      "Loss: 0.01547184493392706\n",
      "Loss: 0.021148383617401123\n",
      "Loss: 0.01797543652355671\n",
      "Loss: 0.016852762550115585\n",
      "Loss: 0.03914982080459595\n",
      "Loss: 0.016416242346167564\n",
      "Loss: 0.04352559894323349\n",
      "Loss: 0.027856962755322456\n",
      "Loss: 0.036727264523506165\n",
      "Loss: 0.023042768239974976\n",
      "Loss: 0.017141049727797508\n",
      "Loss: 0.028055695816874504\n",
      "Loss: 0.01193586178123951\n",
      "Loss: 0.026622438803315163\n",
      "Loss: 0.021184777840971947\n",
      "Loss: 0.03167939931154251\n",
      "Loss: 0.0261111818253994\n",
      "Loss: 0.025959864258766174\n",
      "Loss: 0.0294946376234293\n",
      "Loss: 0.013602571561932564\n",
      "Loss: 0.03239846229553223\n",
      "Loss: 0.03384692966938019\n",
      "Loss: 0.01509862020611763\n",
      "Loss: 0.015600161626935005\n",
      "Loss: 0.03176955506205559\n",
      "Loss: 0.020380988717079163\n",
      "Loss: 0.01641424000263214\n",
      "Loss: 0.017061669379472733\n",
      "Loss: 0.017339712008833885\n",
      "Epoch: 15\n",
      "Loss: 0.018094848841428757\n",
      "Loss: 0.013325310312211514\n",
      "Loss: 0.014006979763507843\n",
      "Loss: 0.015035917982459068\n",
      "Loss: 0.019589096307754517\n",
      "Loss: 0.04101962968707085\n",
      "Loss: 0.019786164164543152\n",
      "Loss: 0.03136228770017624\n",
      "Loss: 0.044314153492450714\n",
      "Loss: 0.02371039241552353\n",
      "Loss: 0.013753360137343407\n",
      "Loss: 0.021027516573667526\n",
      "Loss: 0.02162238210439682\n",
      "Loss: 0.014583912678062916\n",
      "Loss: 0.013677247799932957\n",
      "Loss: 0.028549473732709885\n",
      "Loss: 0.03187942132353783\n",
      "Loss: 0.01921762339770794\n",
      "Loss: 0.016039177775382996\n",
      "Loss: 0.016810590401291847\n",
      "Loss: 0.015634914860129356\n",
      "Loss: 0.017417287454009056\n",
      "Loss: 0.030617106705904007\n",
      "Loss: 0.014276677742600441\n",
      "Loss: 0.008429276756942272\n",
      "Loss: 0.014231545850634575\n",
      "Loss: 0.019294368103146553\n",
      "Loss: 0.014146644622087479\n",
      "Loss: 0.013987651094794273\n",
      "Loss: 0.01285468228161335\n",
      "Loss: 0.027571529150009155\n",
      "Loss: 0.01641187258064747\n",
      "Loss: 0.022225985303521156\n",
      "Loss: 0.016143087297677994\n",
      "Loss: 0.01508374884724617\n",
      "Loss: 0.0120290732011199\n",
      "Loss: 0.02608461119234562\n",
      "Loss: 0.023874454200267792\n",
      "Loss: 0.021055027842521667\n",
      "Loss: 0.024182097986340523\n",
      "Loss: 0.012021719478070736\n",
      "Loss: 0.014471075497567654\n",
      "Loss: 0.012861022725701332\n",
      "Loss: 0.010591572150588036\n",
      "Loss: 0.014970207586884499\n",
      "Loss: 0.022957097738981247\n",
      "Loss: 0.03364873304963112\n",
      "Loss: 0.014064252376556396\n",
      "Loss: 0.014602423645555973\n",
      "Loss: 0.01611759327352047\n",
      "Loss: 0.013214902952313423\n",
      "Epoch: 16\n",
      "Loss: 0.02000081166625023\n",
      "Loss: 0.014599638991057873\n",
      "Loss: 0.013396927155554295\n",
      "Loss: 0.012198897078633308\n",
      "Loss: 0.022779976949095726\n",
      "Loss: 0.00981359463185072\n",
      "Loss: 0.01291962992399931\n",
      "Loss: 0.012858928181231022\n",
      "Loss: 0.012136191129684448\n",
      "Loss: 0.008034906350076199\n",
      "Loss: 0.01562476810067892\n",
      "Loss: 0.018878372386097908\n",
      "Loss: 0.019695186987519264\n",
      "Loss: 0.05477237328886986\n",
      "Loss: 0.012772046960890293\n",
      "Loss: 0.025843355804681778\n",
      "Loss: 0.020837614312767982\n",
      "Loss: 0.017738094553351402\n",
      "Loss: 0.011247913353145123\n",
      "Loss: 0.009243573993444443\n",
      "Loss: 0.014239112846553326\n",
      "Loss: 0.028085295110940933\n",
      "Loss: 0.026860779151320457\n",
      "Loss: 0.017313437536358833\n",
      "Loss: 0.015246473252773285\n",
      "Loss: 0.021341022104024887\n",
      "Loss: 0.012076891027390957\n",
      "Loss: 0.014263597317039967\n",
      "Loss: 0.018352175131440163\n",
      "Loss: 0.012186205014586449\n",
      "Loss: 0.01538037694990635\n",
      "Loss: 0.009110147133469582\n",
      "Loss: 0.014446313492953777\n",
      "Loss: 0.013403193093836308\n",
      "Loss: 0.01563165709376335\n",
      "Loss: 0.018266236409544945\n",
      "Loss: 0.021288443356752396\n",
      "Loss: 0.015336696058511734\n",
      "Loss: 0.012058851309120655\n",
      "Loss: 0.007424736861139536\n",
      "Loss: 0.016982950270175934\n",
      "Loss: 0.013120404444634914\n",
      "Loss: 0.012624996714293957\n",
      "Loss: 0.01235728058964014\n",
      "Loss: 0.019698096439242363\n",
      "Loss: 0.01286123227328062\n",
      "Loss: 0.055922191590070724\n",
      "Loss: 0.014957422390580177\n",
      "Loss: 0.027716994285583496\n",
      "Loss: 0.014792293310165405\n",
      "Loss: 0.019742200151085854\n",
      "Epoch: 17\n",
      "Loss: 0.012474439106881618\n",
      "Loss: 0.01539387833327055\n",
      "Loss: 0.006835411302745342\n",
      "Loss: 0.011392924003303051\n",
      "Loss: 0.010868986137211323\n",
      "Loss: 0.01006240863353014\n",
      "Loss: 0.023593464866280556\n",
      "Loss: 0.03258785232901573\n",
      "Loss: 0.016658440232276917\n",
      "Loss: 0.007238645572215319\n",
      "Loss: 0.014496882446110249\n",
      "Loss: 0.02893696539103985\n",
      "Loss: 0.013101661577820778\n",
      "Loss: 0.02768086828291416\n",
      "Loss: 0.025875817984342575\n",
      "Loss: 0.022434059530496597\n",
      "Loss: 0.01741231046617031\n",
      "Loss: 0.02036202698945999\n",
      "Loss: 0.008425206877291203\n",
      "Loss: 0.014482107944786549\n",
      "Loss: 0.014205900952219963\n",
      "Loss: 0.012276665307581425\n",
      "Loss: 0.004932298790663481\n",
      "Loss: 0.01651763543486595\n",
      "Loss: 0.014396888203918934\n",
      "Loss: 0.01287711039185524\n",
      "Loss: 0.00763872591778636\n",
      "Loss: 0.04043586924672127\n",
      "Loss: 0.006851690821349621\n",
      "Loss: 0.0175604410469532\n",
      "Loss: 0.009061203338205814\n",
      "Loss: 0.01987498812377453\n",
      "Loss: 0.02120790258049965\n",
      "Loss: 0.011440623551607132\n",
      "Loss: 0.011684432625770569\n",
      "Loss: 0.013410446234047413\n",
      "Loss: 0.012502672150731087\n",
      "Loss: 0.008680488914251328\n",
      "Loss: 0.016812767833471298\n",
      "Loss: 0.021543584764003754\n",
      "Loss: 0.013527994975447655\n",
      "Loss: 0.009043215773999691\n",
      "Loss: 0.01255874801427126\n",
      "Loss: 0.012726733461022377\n",
      "Loss: 0.015483926050364971\n",
      "Loss: 0.016480175778269768\n",
      "Loss: 0.008106985129415989\n",
      "Loss: 0.014028001576662064\n",
      "Loss: 0.0137903718277812\n",
      "Loss: 0.010871456004679203\n",
      "Loss: 0.02698914147913456\n",
      "Epoch: 18\n",
      "Loss: 0.00640309602022171\n",
      "Loss: 0.012634579092264175\n",
      "Loss: 0.013026325032114983\n",
      "Loss: 0.008248810656368732\n",
      "Loss: 0.04037522152066231\n",
      "Loss: 0.019729066640138626\n",
      "Loss: 0.017080701887607574\n",
      "Loss: 0.005633573513478041\n",
      "Loss: 0.019011054188013077\n",
      "Loss: 0.009315748699009418\n",
      "Loss: 0.008155466988682747\n",
      "Loss: 0.011932335793972015\n",
      "Loss: 0.02261177822947502\n",
      "Loss: 0.024561811238527298\n",
      "Loss: 0.011112183332443237\n",
      "Loss: 0.004573357291519642\n",
      "Loss: 0.015384003520011902\n",
      "Loss: 0.0052201105281710625\n",
      "Loss: 0.009824952110648155\n",
      "Loss: 0.011291933245956898\n",
      "Loss: 0.006222788710147142\n",
      "Loss: 0.013590398244559765\n",
      "Loss: 0.014537014067173004\n",
      "Loss: 0.0063887303695082664\n",
      "Loss: 0.00564016355201602\n",
      "Loss: 0.017991743981838226\n",
      "Loss: 0.019339926540851593\n",
      "Loss: 0.006094383075833321\n",
      "Loss: 0.010814418084919453\n",
      "Loss: 0.009255219250917435\n",
      "Loss: 0.011094069108366966\n",
      "Loss: 0.00840776227414608\n",
      "Loss: 0.01058551762253046\n",
      "Loss: 0.012620817869901657\n",
      "Loss: 0.033270951360464096\n",
      "Loss: 0.010781648568809032\n",
      "Loss: 0.006092607509344816\n",
      "Loss: 0.008757269941270351\n",
      "Loss: 0.006846822798252106\n",
      "Loss: 0.008996437303721905\n",
      "Loss: 0.020179282873868942\n",
      "Loss: 0.00879654847085476\n",
      "Loss: 0.011279528960585594\n",
      "Loss: 0.013581395149230957\n",
      "Loss: 0.013106694445014\n",
      "Loss: 0.0154776806011796\n",
      "Loss: 0.010655160062015057\n",
      "Loss: 0.00864782277494669\n",
      "Loss: 0.019939690828323364\n",
      "Loss: 0.01589835248887539\n",
      "Loss: 0.016062883660197258\n",
      "Epoch: 19\n",
      "Loss: 0.0043203276582062244\n",
      "Loss: 0.014254491776227951\n",
      "Loss: 0.0066702538169920444\n",
      "Loss: 0.013058262877166271\n",
      "Loss: 0.005116879474371672\n",
      "Loss: 0.015383071266114712\n",
      "Loss: 0.015002346597611904\n",
      "Loss: 0.020051604136824608\n",
      "Loss: 0.009888173080980778\n",
      "Loss: 0.026762252673506737\n",
      "Loss: 0.016361061483621597\n",
      "Loss: 0.006774045526981354\n",
      "Loss: 0.006034500896930695\n",
      "Loss: 0.008309426717460155\n",
      "Loss: 0.006098468787968159\n",
      "Loss: 0.01086015347391367\n",
      "Loss: 0.00989642832428217\n",
      "Loss: 0.011368689127266407\n",
      "Loss: 0.008257675915956497\n",
      "Loss: 0.008238039910793304\n",
      "Loss: 0.011955278925597668\n",
      "Loss: 0.01063825748860836\n",
      "Loss: 0.012458204291760921\n",
      "Loss: 0.008090471848845482\n",
      "Loss: 0.0056646112352609634\n",
      "Loss: 0.005297007970511913\n",
      "Loss: 0.007518179714679718\n",
      "Loss: 0.01712743192911148\n",
      "Loss: 0.016645614057779312\n",
      "Loss: 0.007844177074730396\n",
      "Loss: 0.005051603075116873\n",
      "Loss: 0.006369131151586771\n",
      "Loss: 0.00832054577767849\n",
      "Loss: 0.014367041178047657\n",
      "Loss: 0.003916044719517231\n",
      "Loss: 0.013635432347655296\n",
      "Loss: 0.016515301540493965\n",
      "Loss: 0.009563379921019077\n",
      "Loss: 0.0111929252743721\n",
      "Loss: 0.00740185147151351\n",
      "Loss: 0.004808902274817228\n",
      "Loss: 0.023667190223932266\n",
      "Loss: 0.005061842501163483\n",
      "Loss: 0.013147660531103611\n",
      "Loss: 0.01295572891831398\n",
      "Loss: 0.005719038657844067\n",
      "Loss: 0.006225307937711477\n",
      "Loss: 0.01962984912097454\n",
      "Loss: 0.007662521209567785\n",
      "Loss: 0.005770488642156124\n",
      "Loss: 0.0051013752818107605\n",
      "Epoch: 20\n",
      "Loss: 0.0036443753633648157\n",
      "Loss: 0.011679776012897491\n",
      "Loss: 0.017188498750329018\n",
      "Loss: 0.01748705469071865\n",
      "Loss: 0.023160075768828392\n",
      "Loss: 0.01305917277932167\n",
      "Loss: 0.00810523796826601\n",
      "Loss: 0.0038497180212289095\n",
      "Loss: 0.003166737500578165\n",
      "Loss: 0.00516344653442502\n",
      "Loss: 0.009692671708762646\n",
      "Loss: 0.004598961211740971\n",
      "Loss: 0.011360207572579384\n",
      "Loss: 0.008466104045510292\n",
      "Loss: 0.005561633501201868\n",
      "Loss: 0.009693585336208344\n",
      "Loss: 0.008089547045528889\n",
      "Loss: 0.008091817609965801\n",
      "Loss: 0.0073313782922923565\n",
      "Loss: 0.028418565168976784\n",
      "Loss: 0.00921839103102684\n",
      "Loss: 0.007224760949611664\n",
      "Loss: 0.008760779164731503\n",
      "Loss: 0.021726157516241074\n",
      "Loss: 0.004736560396850109\n",
      "Loss: 0.004766261205077171\n",
      "Loss: 0.007658737245947123\n",
      "Loss: 0.0033335392363369465\n",
      "Loss: 0.004826332908123732\n",
      "Loss: 0.020145848393440247\n",
      "Loss: 0.00988468062132597\n",
      "Loss: 0.010983281768858433\n",
      "Loss: 0.008645233698189259\n",
      "Loss: 0.003967850469052792\n",
      "Loss: 0.0034072573762387037\n",
      "Loss: 0.009210560470819473\n",
      "Loss: 0.00663758534938097\n",
      "Loss: 0.007053013425320387\n",
      "Loss: 0.004260157700628042\n",
      "Loss: 0.005649115890264511\n",
      "Loss: 0.01232486218214035\n",
      "Loss: 0.012071519158780575\n",
      "Loss: 0.007307841908186674\n",
      "Loss: 0.005001220386475325\n",
      "Loss: 0.011687329970300198\n",
      "Loss: 0.005159483291208744\n",
      "Loss: 0.009917247109115124\n",
      "Loss: 0.0056720091961324215\n",
      "Loss: 0.005605148617178202\n",
      "Loss: 0.006426658947020769\n",
      "Loss: 0.0037868546787649393\n",
      "Epoch: 21\n",
      "Loss: 0.003092843806371093\n",
      "Loss: 0.006801221519708633\n",
      "Loss: 0.004385340493172407\n",
      "Loss: 0.0031897288281470537\n",
      "Loss: 0.007817501202225685\n",
      "Loss: 0.007010490633547306\n",
      "Loss: 0.005647724494338036\n",
      "Loss: 0.005431803409010172\n",
      "Loss: 0.00431063724681735\n",
      "Loss: 0.0078046550042927265\n",
      "Loss: 0.007198097184300423\n",
      "Loss: 0.010172576643526554\n",
      "Loss: 0.003338239388540387\n",
      "Loss: 0.006225882098078728\n",
      "Loss: 0.007020095828920603\n",
      "Loss: 0.0052786883898079395\n",
      "Loss: 0.005307249259203672\n",
      "Loss: 0.003319334937259555\n",
      "Loss: 0.005703954491764307\n",
      "Loss: 0.007349865511059761\n",
      "Loss: 0.007802859414368868\n",
      "Loss: 0.011501405388116837\n",
      "Loss: 0.004891504533588886\n",
      "Loss: 0.006553127430379391\n",
      "Loss: 0.006397017743438482\n",
      "Loss: 0.006811434868723154\n",
      "Loss: 0.005057964939624071\n",
      "Loss: 0.009224171750247478\n",
      "Loss: 0.02021286077797413\n",
      "Loss: 0.015045168809592724\n",
      "Loss: 0.004534461535513401\n",
      "Loss: 0.008638973347842693\n",
      "Loss: 0.011453199200332165\n",
      "Loss: 0.0034137011971324682\n",
      "Loss: 0.0039220754988491535\n",
      "Loss: 0.004144643433392048\n",
      "Loss: 0.008366248570382595\n",
      "Loss: 0.01507668849080801\n",
      "Loss: 0.005955095402896404\n",
      "Loss: 0.01855679228901863\n",
      "Loss: 0.003400935558602214\n",
      "Loss: 0.003946880344301462\n",
      "Loss: 0.005675334949046373\n",
      "Loss: 0.008236756548285484\n",
      "Loss: 0.012089896947145462\n",
      "Loss: 0.006279598921537399\n",
      "Loss: 0.0037655839696526527\n",
      "Loss: 0.005463267210870981\n",
      "Loss: 0.005091554950922728\n",
      "Loss: 0.012168115936219692\n",
      "Loss: 0.003048352897167206\n",
      "Epoch: 22\n",
      "Loss: 0.0037690361496061087\n",
      "Loss: 0.0031649956945329905\n",
      "Loss: 0.006577009800821543\n",
      "Loss: 0.0032188643235713243\n",
      "Loss: 0.00478419428691268\n",
      "Loss: 0.011048720218241215\n",
      "Loss: 0.004966682754456997\n",
      "Loss: 0.005793567281216383\n",
      "Loss: 0.005526628810912371\n",
      "Loss: 0.003300301730632782\n",
      "Loss: 0.007903432473540306\n",
      "Loss: 0.0106755830347538\n",
      "Loss: 0.0027753172907978296\n",
      "Loss: 0.003952142782509327\n",
      "Loss: 0.0033590118400752544\n",
      "Loss: 0.004364050459116697\n",
      "Loss: 0.004571377299726009\n",
      "Loss: 0.0035976117942482233\n",
      "Loss: 0.002728275256231427\n",
      "Loss: 0.003249124391004443\n",
      "Loss: 0.007169274613261223\n",
      "Loss: 0.0031585253309458494\n",
      "Loss: 0.010809026658535004\n",
      "Loss: 0.011892561800777912\n",
      "Loss: 0.005055844318121672\n",
      "Loss: 0.008938169106841087\n",
      "Loss: 0.004613487049937248\n",
      "Loss: 0.006931913550943136\n",
      "Loss: 0.006657457444816828\n",
      "Loss: 0.0041761016473174095\n",
      "Loss: 0.012288806028664112\n",
      "Loss: 0.004765230696648359\n",
      "Loss: 0.0072809262201189995\n",
      "Loss: 0.009344134479761124\n",
      "Loss: 0.003183882450684905\n",
      "Loss: 0.008885606192052364\n",
      "Loss: 0.0033352673053741455\n",
      "Loss: 0.004907821770757437\n",
      "Loss: 0.005548045039176941\n",
      "Loss: 0.004845189396291971\n",
      "Loss: 0.0034108103718608618\n",
      "Loss: 0.008761486038565636\n",
      "Loss: 0.008696137927472591\n",
      "Loss: 0.015275784768164158\n",
      "Loss: 0.003997877240180969\n",
      "Loss: 0.00423166761174798\n",
      "Loss: 0.006415202748030424\n",
      "Loss: 0.0035330404061824083\n",
      "Loss: 0.0026719621382653713\n",
      "Loss: 0.009247572161257267\n",
      "Loss: 0.003882552497088909\n",
      "Epoch: 23\n",
      "Loss: 0.004896349739283323\n",
      "Loss: 0.004539164248853922\n",
      "Loss: 0.002798302797600627\n",
      "Loss: 0.0027258091140538454\n",
      "Loss: 0.003068656660616398\n",
      "Loss: 0.007513630669564009\n",
      "Loss: 0.002701039193198085\n",
      "Loss: 0.002943367464467883\n",
      "Loss: 0.0023912563920021057\n",
      "Loss: 0.0061009605415165424\n",
      "Loss: 0.0030441288836300373\n",
      "Loss: 0.003307014238089323\n",
      "Loss: 0.0026180243585258722\n",
      "Loss: 0.006478770170360804\n",
      "Loss: 0.0064917816780507565\n",
      "Loss: 0.002790149301290512\n",
      "Loss: 0.0026611650828272104\n",
      "Loss: 0.003463353030383587\n",
      "Loss: 0.004646195564419031\n",
      "Loss: 0.0038896864280104637\n",
      "Loss: 0.005901914089918137\n",
      "Loss: 0.0027379258535802364\n",
      "Loss: 0.003772030584514141\n",
      "Loss: 0.004200939554721117\n",
      "Loss: 0.0033237291499972343\n",
      "Loss: 0.0030321895610541105\n",
      "Loss: 0.0027492076624184847\n",
      "Loss: 0.02241981402039528\n",
      "Loss: 0.002446515718474984\n",
      "Loss: 0.0037669935263693333\n",
      "Loss: 0.0036749171558767557\n",
      "Loss: 0.003168096300214529\n",
      "Loss: 0.0054261949844658375\n",
      "Loss: 0.00323239853605628\n",
      "Loss: 0.005665450356900692\n",
      "Loss: 0.0025345266330987215\n",
      "Loss: 0.009365607984364033\n",
      "Loss: 0.010142125189304352\n",
      "Loss: 0.007973521947860718\n",
      "Loss: 0.004927016794681549\n",
      "Loss: 0.0032113371416926384\n",
      "Loss: 0.004299449734389782\n",
      "Loss: 0.005351087544113398\n",
      "Loss: 0.0038819604087620974\n",
      "Loss: 0.0024727953132241964\n",
      "Loss: 0.007568118162453175\n",
      "Loss: 0.006991126108914614\n",
      "Loss: 0.021288175135850906\n",
      "Loss: 0.0025561454240232706\n",
      "Loss: 0.003380883950740099\n",
      "Loss: 0.00337245617993176\n",
      "Epoch: 24\n",
      "Loss: 0.0030589306261390448\n",
      "Loss: 0.006563604809343815\n",
      "Loss: 0.0031350431963801384\n",
      "Loss: 0.004616281017661095\n",
      "Loss: 0.004077906720340252\n",
      "Loss: 0.002413650508970022\n",
      "Loss: 0.005811658222228289\n",
      "Loss: 0.0070630526170134544\n",
      "Loss: 0.002780589973554015\n",
      "Loss: 0.008255496621131897\n",
      "Loss: 0.0050896573811769485\n",
      "Loss: 0.002402088837698102\n",
      "Loss: 0.004248885903507471\n",
      "Loss: 0.0031211739405989647\n",
      "Loss: 0.01258418895304203\n",
      "Loss: 0.004973130766302347\n",
      "Loss: 0.0025248355232179165\n",
      "Loss: 0.0021816145163029432\n",
      "Loss: 0.00295261200517416\n",
      "Loss: 0.0024518389254808426\n",
      "Loss: 0.00664543965831399\n",
      "Loss: 0.0038399170152843\n",
      "Loss: 0.0027168369852006435\n",
      "Loss: 0.005508652422577143\n",
      "Loss: 0.008597295731306076\n",
      "Loss: 0.003156111342832446\n",
      "Loss: 0.0030254304874688387\n",
      "Loss: 0.0027693570591509342\n",
      "Loss: 0.002453940687701106\n",
      "Loss: 0.011303753592073917\n",
      "Loss: 0.0045099700801074505\n",
      "Loss: 0.003889472456648946\n",
      "Loss: 0.002628146670758724\n",
      "Loss: 0.002210285048931837\n",
      "Loss: 0.0036908327601850033\n",
      "Loss: 0.009572049602866173\n",
      "Loss: 0.004878100007772446\n",
      "Loss: 0.002927647205069661\n",
      "Loss: 0.002805212279781699\n",
      "Loss: 0.0031133899465203285\n",
      "Loss: 0.0022647627629339695\n",
      "Loss: 0.002221393398940563\n",
      "Loss: 0.005211237818002701\n",
      "Loss: 0.0023985623847693205\n",
      "Loss: 0.0020283397752791643\n",
      "Loss: 0.0023788653779774904\n",
      "Loss: 0.002337022451683879\n",
      "Loss: 0.002228419529274106\n",
      "Loss: 0.004036400001496077\n",
      "Loss: 0.0021180298645049334\n",
      "Loss: 0.0037850942462682724\n",
      "Epoch: 25\n",
      "Loss: 0.002100531244650483\n",
      "Loss: 0.004293275065720081\n",
      "Loss: 0.00821787677705288\n",
      "Loss: 0.007161642890423536\n",
      "Loss: 0.0026773710269480944\n",
      "Loss: 0.0027147557120770216\n",
      "Loss: 0.0020850510336458683\n",
      "Loss: 0.006658213213086128\n",
      "Loss: 0.006256332155317068\n",
      "Loss: 0.002605405170470476\n",
      "Loss: 0.0022537708282470703\n",
      "Loss: 0.004734598565846682\n",
      "Loss: 0.0022802355233579874\n",
      "Loss: 0.002046092413365841\n",
      "Loss: 0.002780101727694273\n",
      "Loss: 0.011386985890567303\n",
      "Loss: 0.0025924923829734325\n",
      "Loss: 0.0023225012701004744\n",
      "Loss: 0.002409338718280196\n",
      "Loss: 0.007199036423116922\n",
      "Loss: 0.00261099636554718\n",
      "Loss: 0.0036164994817227125\n",
      "Loss: 0.0038087028078734875\n",
      "Loss: 0.0022777903359383345\n",
      "Loss: 0.0033980889711529016\n",
      "Loss: 0.002004866022616625\n",
      "Loss: 0.0019734781235456467\n",
      "Loss: 0.0072072166949510574\n",
      "Loss: 0.002033446915447712\n",
      "Loss: 0.004961955826729536\n",
      "Loss: 0.006241840776056051\n",
      "Loss: 0.004427401814609766\n",
      "Loss: 0.006482883356511593\n",
      "Loss: 0.00192350207362324\n",
      "Loss: 0.0036982104647904634\n",
      "Loss: 0.0027881013229489326\n",
      "Loss: 0.0021981517784297466\n",
      "Loss: 0.0037078692112118006\n",
      "Loss: 0.00338940043002367\n",
      "Loss: 0.0020523499697446823\n",
      "Loss: 0.0022209063172340393\n",
      "Loss: 0.0020571674685925245\n",
      "Loss: 0.0022004141937941313\n",
      "Loss: 0.004023024346679449\n",
      "Loss: 0.004531789105385542\n",
      "Loss: 0.0032712360844016075\n",
      "Loss: 0.002039173385128379\n",
      "Loss: 0.0021778277587145567\n",
      "Loss: 0.0019993348978459835\n",
      "Loss: 0.007212440948933363\n",
      "Loss: 0.0023917709477245808\n",
      "Epoch: 26\n",
      "Loss: 0.006576090585440397\n",
      "Loss: 0.0034997579641640186\n",
      "Loss: 0.007021150551736355\n",
      "Loss: 0.0020204277243465185\n",
      "Loss: 0.0019341233419254422\n",
      "Loss: 0.006950445473194122\n",
      "Loss: 0.005163418594747782\n",
      "Loss: 0.0019882861524820328\n",
      "Loss: 0.001957325264811516\n",
      "Loss: 0.003665229305624962\n",
      "Loss: 0.002101548481732607\n",
      "Loss: 0.003948544152081013\n",
      "Loss: 0.001949659432284534\n",
      "Loss: 0.002644159598276019\n",
      "Loss: 0.0027600021567195654\n",
      "Loss: 0.002449990250170231\n",
      "Loss: 0.0019438904710114002\n",
      "Loss: 0.002055489458143711\n",
      "Loss: 0.002784573473036289\n",
      "Loss: 0.007208474446088076\n",
      "Loss: 0.004187366459518671\n",
      "Loss: 0.0021442004945129156\n",
      "Loss: 0.006344568450003862\n",
      "Loss: 0.002237841719761491\n",
      "Loss: 0.006076649762690067\n",
      "Loss: 0.003698997898027301\n",
      "Loss: 0.0019114528549835086\n",
      "Loss: 0.002043867716565728\n",
      "Loss: 0.0020987174939364195\n",
      "Loss: 0.007745606359094381\n",
      "Loss: 0.002250368008390069\n",
      "Loss: 0.01345757395029068\n",
      "Loss: 0.002033743541687727\n",
      "Loss: 0.002222801558673382\n",
      "Loss: 0.002296195365488529\n",
      "Loss: 0.0019421032629907131\n",
      "Loss: 0.006362488493323326\n",
      "Loss: 0.0024657314643263817\n",
      "Loss: 0.0068836575374007225\n",
      "Loss: 0.002782990923151374\n",
      "Loss: 0.0018753738841041923\n",
      "Loss: 0.00459870882332325\n",
      "Loss: 0.005227383226156235\n",
      "Loss: 0.0018076524138450623\n",
      "Loss: 0.0038207422476261854\n",
      "Loss: 0.009139484725892544\n",
      "Loss: 0.0023674173280596733\n",
      "Loss: 0.001976881641894579\n",
      "Loss: 0.001797709846869111\n",
      "Loss: 0.002430155873298645\n",
      "Loss: 0.0018894232343882322\n",
      "Epoch: 27\n",
      "Loss: 0.0019826670177280903\n",
      "Loss: 0.009111515246331692\n",
      "Loss: 0.002139823976904154\n",
      "Loss: 0.00923212431371212\n",
      "Loss: 0.010853417217731476\n",
      "Loss: 0.0033147367648780346\n",
      "Loss: 0.005391515325754881\n",
      "Loss: 0.004049329087138176\n",
      "Loss: 0.002107110107317567\n",
      "Loss: 0.00203980621881783\n",
      "Loss: 0.0019107275875285268\n",
      "Loss: 0.010787218809127808\n",
      "Loss: 0.00182448816485703\n",
      "Loss: 0.0025121450889855623\n",
      "Loss: 0.002026912523433566\n",
      "Loss: 0.007785264868289232\n",
      "Loss: 0.00390880461782217\n",
      "Loss: 0.0021410274785012007\n",
      "Loss: 0.0052700042724609375\n",
      "Loss: 0.007101913448423147\n",
      "Loss: 0.002261703135445714\n",
      "Loss: 0.011986277997493744\n",
      "Loss: 0.002067890716716647\n",
      "Loss: 0.001955306390300393\n",
      "Loss: 0.0018216520547866821\n",
      "Loss: 0.0018645399250090122\n",
      "Loss: 0.004681276623159647\n",
      "Loss: 0.001765595399774611\n",
      "Loss: 0.0030283445958048105\n",
      "Loss: 0.0018140710890293121\n",
      "Loss: 0.002499200636520982\n",
      "Loss: 0.004331228323280811\n",
      "Loss: 0.0017745491350069642\n",
      "Loss: 0.004913882818073034\n",
      "Loss: 0.0019160585943609476\n",
      "Loss: 0.005245310254395008\n",
      "Loss: 0.0018539450829848647\n",
      "Loss: 0.0018555523129180074\n",
      "Loss: 0.003311245935037732\n",
      "Loss: 0.0025742575526237488\n",
      "Loss: 0.0020401885267347097\n",
      "Loss: 0.0022272570058703423\n",
      "Loss: 0.0062890928238630295\n",
      "Loss: 0.0019024708308279514\n",
      "Loss: 0.011849851347506046\n",
      "Loss: 0.0017766171367838979\n",
      "Loss: 0.0024938497226685286\n",
      "Loss: 0.0023122397251427174\n",
      "Loss: 0.006013925652951002\n",
      "Loss: 0.002315924037247896\n",
      "Loss: 0.0018002245342358947\n",
      "Epoch: 28\n",
      "Loss: 0.0020910478197038174\n",
      "Loss: 0.002722325036302209\n",
      "Loss: 0.025868894532322884\n",
      "Loss: 0.0023557899985462427\n",
      "Loss: 0.002501242095604539\n",
      "Loss: 0.0019493061117827892\n",
      "Loss: 0.0030127689242362976\n",
      "Loss: 0.0050159841775894165\n",
      "Loss: 0.0020254068076610565\n",
      "Loss: 0.02120727300643921\n",
      "Loss: 0.005084860138595104\n",
      "Loss: 0.0017749440157786012\n",
      "Loss: 0.003980094101279974\n",
      "Loss: 0.0024932785890996456\n",
      "Loss: 0.0023604922462254763\n",
      "Loss: 0.005301544442772865\n",
      "Loss: 0.0017134920926764607\n",
      "Loss: 0.014239287935197353\n",
      "Loss: 0.002876712242141366\n",
      "Loss: 0.0017530504846945405\n",
      "Loss: 0.002116467570886016\n",
      "Loss: 0.003134960774332285\n",
      "Loss: 0.037012260407209396\n",
      "Loss: 0.005150620359927416\n",
      "Loss: 0.002336842240765691\n",
      "Loss: 0.0019671840127557516\n",
      "Loss: 0.003359540831297636\n",
      "Loss: 0.00225110724568367\n",
      "Loss: 0.0029158464167267084\n",
      "Loss: 0.0025369205977767706\n",
      "Loss: 0.00275665195658803\n",
      "Loss: 0.008193091489374638\n",
      "Loss: 0.0021975731942802668\n",
      "Loss: 0.004790504928678274\n",
      "Loss: 0.0035690513905137777\n",
      "Loss: 0.004282547626644373\n",
      "Loss: 0.0024306303821504116\n",
      "Loss: 0.006500286515802145\n",
      "Loss: 0.005810352507978678\n",
      "Loss: 0.0021221977658569813\n",
      "Loss: 0.0017723386408761144\n",
      "Loss: 0.0026453270111232996\n",
      "Loss: 0.004997230600565672\n",
      "Loss: 0.006156756542623043\n",
      "Loss: 0.0018235048046335578\n",
      "Loss: 0.002158647170290351\n",
      "Loss: 0.0035526459105312824\n",
      "Loss: 0.003286971477791667\n",
      "Loss: 0.0019082101061940193\n",
      "Loss: 0.0018911893712356687\n",
      "Loss: 0.0020755743607878685\n",
      "Epoch: 29\n",
      "Loss: 0.007796302437782288\n",
      "Loss: 0.011789890937507153\n",
      "Loss: 0.0031580845825374126\n",
      "Loss: 0.002222590846940875\n",
      "Loss: 0.0023653495591133833\n",
      "Loss: 0.0018030183855444193\n",
      "Loss: 0.0017718258313834667\n",
      "Loss: 0.0034249122254550457\n",
      "Loss: 0.004139219410717487\n",
      "Loss: 0.018170896917581558\n",
      "Loss: 0.0018483249004930258\n",
      "Loss: 0.001963405404239893\n",
      "Loss: 0.0025036188308149576\n",
      "Loss: 0.001821230980567634\n",
      "Loss: 0.004517924040555954\n",
      "Loss: 0.0019017986487597227\n",
      "Loss: 0.0017727918457239866\n",
      "Loss: 0.003948582336306572\n",
      "Loss: 0.004593777470290661\n",
      "Loss: 0.0016697305254638195\n",
      "Loss: 0.0018061610171571374\n",
      "Loss: 0.0015645669773221016\n",
      "Loss: 0.004784796852618456\n",
      "Loss: 0.0036315054167062044\n",
      "Loss: 0.0016574759501963854\n",
      "Loss: 0.0034826193004846573\n",
      "Loss: 0.0017745515797287226\n",
      "Loss: 0.002074674004688859\n",
      "Loss: 0.002024264307692647\n",
      "Loss: 0.0015894330572336912\n",
      "Loss: 0.005502288229763508\n",
      "Loss: 0.0018974863924086094\n",
      "Loss: 0.0019257209496572614\n",
      "Loss: 0.0061434595845639706\n",
      "Loss: 0.002052817726507783\n",
      "Loss: 0.0022392908576875925\n",
      "Loss: 0.001936074928380549\n",
      "Loss: 0.013003616593778133\n",
      "Loss: 0.0017855431651696563\n",
      "Loss: 0.0019052891293540597\n",
      "Loss: 0.0014887845609337091\n",
      "Loss: 0.027983983978629112\n",
      "Loss: 0.006621356122195721\n",
      "Loss: 0.0026963420677930117\n",
      "Loss: 0.008741495199501514\n",
      "Loss: 0.010116202756762505\n",
      "Loss: 0.0019504460506141186\n",
      "Loss: 0.0016795689007267356\n",
      "Loss: 0.006260473746806383\n",
      "Loss: 0.0017112225759774446\n",
      "Loss: 0.003290353110060096\n",
      "Epoch: 30\n",
      "Loss: 0.002734611975029111\n",
      "Loss: 0.0016413903795182705\n",
      "Loss: 0.0015939852455630898\n",
      "Loss: 0.002021998167037964\n",
      "Loss: 0.006843388546258211\n",
      "Loss: 0.0022042617201805115\n",
      "Loss: 0.0035276422277092934\n",
      "Loss: 0.009180196560919285\n",
      "Loss: 0.002138512209057808\n",
      "Loss: 0.001634927117265761\n",
      "Loss: 0.017358599230647087\n",
      "Loss: 0.002365491585806012\n",
      "Loss: 0.00262429635040462\n",
      "Loss: 0.001752294017933309\n",
      "Loss: 0.0025456261355429888\n",
      "Loss: 0.0017250783275812864\n",
      "Loss: 0.0016428428934887052\n",
      "Loss: 0.0018530702218413353\n",
      "Loss: 0.0026835633907467127\n",
      "Loss: 0.0015000560088083148\n",
      "Loss: 0.0021431781351566315\n",
      "Loss: 0.003191395429894328\n",
      "Loss: 0.00370450085029006\n",
      "Loss: 0.0018520914018154144\n",
      "Loss: 0.0019126508850604296\n",
      "Loss: 0.0021046071778982878\n",
      "Loss: 0.0038772982079535723\n",
      "Loss: 0.002691165544092655\n",
      "Loss: 0.002051801420748234\n",
      "Loss: 0.002083521569147706\n",
      "Loss: 0.0017228632932528853\n",
      "Loss: 0.005039896350353956\n",
      "Loss: 0.02069264091551304\n",
      "Loss: 0.008263778872787952\n",
      "Loss: 0.002632320625707507\n",
      "Loss: 0.0034138476476073265\n",
      "Loss: 0.00147396104875952\n",
      "Loss: 0.001706853974610567\n",
      "Loss: 0.0038120336830615997\n",
      "Loss: 0.0020746984519064426\n",
      "Loss: 0.0044588311575353146\n",
      "Loss: 0.005357224494218826\n",
      "Loss: 0.004778795875608921\n",
      "Loss: 0.0015715253539383411\n",
      "Loss: 0.003933061379939318\n",
      "Loss: 0.0030102296732366085\n",
      "Loss: 0.00961202010512352\n",
      "Loss: 0.004380167927592993\n",
      "Loss: 0.0035329724196344614\n",
      "Loss: 0.011000352911651134\n",
      "Loss: 0.0033647918608039618\n",
      "Epoch: 31\n",
      "Loss: 0.0019110201392322779\n",
      "Loss: 0.0017488549929112196\n",
      "Loss: 0.002312371740117669\n",
      "Loss: 0.013391903601586819\n",
      "Loss: 0.002220070455223322\n",
      "Loss: 0.003096639644354582\n",
      "Loss: 0.006448768079280853\n",
      "Loss: 0.0036791996099054813\n",
      "Loss: 0.004845816642045975\n",
      "Loss: 0.0022789607755839825\n",
      "Loss: 0.0019076670287176967\n",
      "Loss: 0.0022499384358525276\n",
      "Loss: 0.008745744824409485\n",
      "Loss: 0.006254210602492094\n",
      "Loss: 0.008563109673559666\n",
      "Loss: 0.002687401371076703\n",
      "Loss: 0.00558781111612916\n",
      "Loss: 0.004121547564864159\n",
      "Loss: 0.005117332562804222\n",
      "Loss: 0.013094699010252953\n",
      "Loss: 0.009651167318224907\n",
      "Loss: 0.003560706041753292\n",
      "Loss: 0.004065060056746006\n",
      "Loss: 0.0024534238036721945\n",
      "Loss: 0.0074548679403960705\n",
      "Loss: 0.001581451972015202\n",
      "Loss: 0.00276688439771533\n",
      "Loss: 0.006353978533297777\n",
      "Loss: 0.004389188718050718\n",
      "Loss: 0.0046959989704191685\n",
      "Loss: 0.007545141968876123\n",
      "Loss: 0.0038365956861525774\n",
      "Loss: 0.002701860386878252\n",
      "Loss: 0.0014242675388231874\n",
      "Loss: 0.0024821790866553783\n",
      "Loss: 0.0029893291648477316\n",
      "Loss: 0.009503021836280823\n",
      "Loss: 0.003608613507822156\n",
      "Loss: 0.008850036188960075\n",
      "Loss: 0.005204112734645605\n",
      "Loss: 0.00496403593569994\n",
      "Loss: 0.004670784343034029\n",
      "Loss: 0.006136524491012096\n",
      "Loss: 0.004723606631159782\n",
      "Loss: 0.005040117539465427\n",
      "Loss: 0.008642070926725864\n",
      "Loss: 0.007399996742606163\n",
      "Loss: 0.007319421507418156\n",
      "Loss: 0.010009068064391613\n",
      "Loss: 0.0022066733799874783\n",
      "Loss: 0.0026607115287333727\n",
      "Epoch: 32\n",
      "Loss: 0.005568776745349169\n",
      "Loss: 0.008619001135230064\n",
      "Loss: 0.010929376818239689\n",
      "Loss: 0.0026838991325348616\n",
      "Loss: 0.007415404077619314\n",
      "Loss: 0.001871550572104752\n",
      "Loss: 0.0063351853750646114\n",
      "Loss: 0.00934622809290886\n",
      "Loss: 0.0029210106004029512\n",
      "Loss: 0.0024510733783245087\n",
      "Loss: 0.009548568166792393\n",
      "Loss: 0.005870264954864979\n",
      "Loss: 0.00564568629488349\n",
      "Loss: 0.007415316998958588\n",
      "Loss: 0.015630409121513367\n",
      "Loss: 0.0021093899849802256\n",
      "Loss: 0.013978397473692894\n",
      "Loss: 0.009420895017683506\n",
      "Loss: 0.005532837472856045\n",
      "Loss: 0.003693373640999198\n",
      "Loss: 0.008923501707613468\n",
      "Loss: 0.006151185370981693\n",
      "Loss: 0.002071145223453641\n",
      "Loss: 0.004649718292057514\n",
      "Loss: 0.007339994423091412\n",
      "Loss: 0.007702641189098358\n",
      "Loss: 0.00446984451264143\n",
      "Loss: 0.004589076153934002\n",
      "Loss: 0.007264760788530111\n",
      "Loss: 0.0042474581860005856\n",
      "Loss: 0.005137647967785597\n",
      "Loss: 0.005042222328484058\n",
      "Loss: 0.013631482608616352\n",
      "Loss: 0.003586665727198124\n",
      "Loss: 0.015750695019960403\n",
      "Loss: 0.014259361661970615\n",
      "Loss: 0.003922642674297094\n",
      "Loss: 0.0048413388431072235\n",
      "Loss: 0.013652215711772442\n",
      "Loss: 0.008806395344436169\n",
      "Loss: 0.0038790060207247734\n",
      "Loss: 0.006409873720258474\n",
      "Loss: 0.009382196702063084\n",
      "Loss: 0.013157747685909271\n",
      "Loss: 0.0059390440583229065\n",
      "Loss: 0.011386736296117306\n",
      "Loss: 0.011483719572424889\n",
      "Loss: 0.010010959580540657\n",
      "Loss: 0.005519206169992685\n",
      "Loss: 0.008456191048026085\n",
      "Loss: 0.0028577428311109543\n",
      "Epoch: 33\n",
      "Loss: 0.0021636877208948135\n",
      "Loss: 0.004294409416615963\n",
      "Loss: 0.002757501322776079\n",
      "Loss: 0.003039383562281728\n",
      "Loss: 0.00765569880604744\n",
      "Loss: 0.004060728475451469\n",
      "Loss: 0.0033170590177178383\n",
      "Loss: 0.003802764695137739\n",
      "Loss: 0.007365004625171423\n",
      "Loss: 0.0016895459266379476\n",
      "Loss: 0.002005924703553319\n",
      "Loss: 0.001549682579934597\n",
      "Loss: 0.002388338791206479\n",
      "Loss: 0.0047867330722510815\n",
      "Loss: 0.005088770762085915\n",
      "Loss: 0.004358045756816864\n",
      "Loss: 0.00844548549503088\n",
      "Loss: 0.005764946807175875\n",
      "Loss: 0.00300738331861794\n",
      "Loss: 0.0018924908945336938\n",
      "Loss: 0.007639038376510143\n",
      "Loss: 0.005181890446692705\n",
      "Loss: 0.0037382955197244883\n",
      "Loss: 0.004997138865292072\n",
      "Loss: 0.0038806262891739607\n",
      "Loss: 0.0016089484561234713\n",
      "Loss: 0.014201806858181953\n",
      "Loss: 0.007375187706202269\n",
      "Loss: 0.008835255168378353\n",
      "Loss: 0.0031177240889519453\n",
      "Loss: 0.01029865350574255\n",
      "Loss: 0.0020420874934643507\n",
      "Loss: 0.006612011231482029\n",
      "Loss: 0.009315576404333115\n",
      "Loss: 0.007616518065333366\n",
      "Loss: 0.009246397763490677\n",
      "Loss: 0.013818500563502312\n",
      "Loss: 0.00726328045129776\n",
      "Loss: 0.012147285044193268\n",
      "Loss: 0.006618797313421965\n",
      "Loss: 0.013253205455839634\n",
      "Loss: 0.009081676602363586\n",
      "Loss: 0.006236810237169266\n",
      "Loss: 0.008460009470582008\n",
      "Loss: 0.005504505708813667\n",
      "Loss: 0.007733512669801712\n",
      "Loss: 0.012713410891592503\n",
      "Loss: 0.006084461696445942\n",
      "Loss: 0.00631515309214592\n",
      "Loss: 0.014600834809243679\n",
      "Loss: 0.007208292372524738\n",
      "Epoch: 34\n",
      "Loss: 0.007664805743843317\n",
      "Loss: 0.010980479419231415\n",
      "Loss: 0.004330049268901348\n",
      "Loss: 0.006634763907641172\n",
      "Loss: 0.0072480738162994385\n",
      "Loss: 0.004119902849197388\n",
      "Loss: 0.006814533844590187\n",
      "Loss: 0.0037025478668510914\n",
      "Loss: 0.007793006487190723\n",
      "Loss: 0.004781811963766813\n",
      "Loss: 0.003766942536458373\n",
      "Loss: 0.005327367223799229\n",
      "Loss: 0.0014920677058398724\n",
      "Loss: 0.0021824869327247143\n",
      "Loss: 0.007781909313052893\n",
      "Loss: 0.00199555023573339\n",
      "Loss: 0.0038109002634882927\n",
      "Loss: 0.002064042491838336\n",
      "Loss: 0.006698157638311386\n",
      "Loss: 0.008292649872601032\n",
      "Loss: 0.0030717807821929455\n",
      "Loss: 0.0061569432727992535\n",
      "Loss: 0.007579365745186806\n",
      "Loss: 0.007076029200106859\n",
      "Loss: 0.005317134782671928\n",
      "Loss: 0.008053689263761044\n",
      "Loss: 0.0021059622522443533\n",
      "Loss: 0.0034942850470542908\n",
      "Loss: 0.002572514582425356\n",
      "Loss: 0.007763594388961792\n",
      "Loss: 0.00460843276232481\n",
      "Loss: 0.0023271345999091864\n",
      "Loss: 0.0036995660047978163\n",
      "Loss: 0.004132779315114021\n",
      "Loss: 0.002284975489601493\n",
      "Loss: 0.003183281747624278\n",
      "Loss: 0.014045813120901585\n",
      "Loss: 0.0028841758612543344\n",
      "Loss: 0.004759626928716898\n",
      "Loss: 0.0027592985425144434\n",
      "Loss: 0.005466494709253311\n",
      "Loss: 0.0034212442114949226\n",
      "Loss: 0.0025669641327112913\n",
      "Loss: 0.0052049127407372\n",
      "Loss: 0.0034249324817210436\n",
      "Loss: 0.013213698752224445\n",
      "Loss: 0.0111265629529953\n",
      "Loss: 0.0037150506395846605\n",
      "Loss: 0.0027553141117095947\n",
      "Loss: 0.005039615090936422\n",
      "Loss: 0.010668309405446053\n",
      "Epoch: 35\n",
      "Loss: 0.002284511225298047\n",
      "Loss: 0.0018836776725947857\n",
      "Loss: 0.0042567471973598\n",
      "Loss: 0.0013365764170885086\n",
      "Loss: 0.002603714819997549\n",
      "Loss: 0.007517275866121054\n",
      "Loss: 0.003845770610496402\n",
      "Loss: 0.0012916710693389177\n",
      "Loss: 0.004624564666301012\n",
      "Loss: 0.006113733630627394\n",
      "Loss: 0.003715026658028364\n",
      "Loss: 0.004717446863651276\n",
      "Loss: 0.002911058021709323\n",
      "Loss: 0.002073094714432955\n",
      "Loss: 0.003756096586585045\n",
      "Loss: 0.0019047068199142814\n",
      "Loss: 0.004148545209318399\n",
      "Loss: 0.002989586442708969\n",
      "Loss: 0.002105558989569545\n",
      "Loss: 0.0012687351554632187\n",
      "Loss: 0.0013539745705202222\n",
      "Loss: 0.005677192471921444\n",
      "Loss: 0.0022880989126861095\n",
      "Loss: 0.0014316493179649115\n",
      "Loss: 0.0020815383177250624\n",
      "Loss: 0.0013345186598598957\n",
      "Loss: 0.0014072720659896731\n",
      "Loss: 0.004550573416054249\n",
      "Loss: 0.004078957252204418\n",
      "Loss: 0.002713755937293172\n",
      "Loss: 0.0014982240973040462\n",
      "Loss: 0.00431456696242094\n",
      "Loss: 0.005963902920484543\n",
      "Loss: 0.0014934695791453123\n",
      "Loss: 0.0019289396004751325\n",
      "Loss: 0.002899323822930455\n",
      "Loss: 0.0016156112542375922\n",
      "Loss: 0.002158521441742778\n",
      "Loss: 0.0023717230651527643\n",
      "Loss: 0.0013724673772230744\n",
      "Loss: 0.0036614742130041122\n",
      "Loss: 0.0015885919565334916\n",
      "Loss: 0.0033402596600353718\n",
      "Loss: 0.006214800290763378\n",
      "Loss: 0.004649055656045675\n",
      "Loss: 0.0029500918462872505\n",
      "Loss: 0.002517946530133486\n",
      "Loss: 0.0014958014944568276\n",
      "Loss: 0.0017469696467742324\n",
      "Loss: 0.0014104051515460014\n",
      "Loss: 0.002168141771107912\n",
      "Epoch: 36\n",
      "Loss: 0.002461040858179331\n",
      "Loss: 0.0031935644801706076\n",
      "Loss: 0.007961582392454147\n",
      "Loss: 0.0015388880856335163\n",
      "Loss: 0.0025875773280858994\n",
      "Loss: 0.0017996549140661955\n",
      "Loss: 0.0017511100741103292\n",
      "Loss: 0.005687471013516188\n",
      "Loss: 0.0012838981347158551\n",
      "Loss: 0.002313054632395506\n",
      "Loss: 0.001399288885295391\n",
      "Loss: 0.0036053985822945833\n",
      "Loss: 0.0020834999158978462\n",
      "Loss: 0.002050301292911172\n",
      "Loss: 0.0013638270320370793\n",
      "Loss: 0.001815200550481677\n",
      "Loss: 0.0013368198415264487\n",
      "Loss: 0.0014316060114651918\n",
      "Loss: 0.0023654622491449118\n",
      "Loss: 0.0016240336699411273\n",
      "Loss: 0.003985218703746796\n",
      "Loss: 0.003564812708646059\n",
      "Loss: 0.0021484175231307745\n",
      "Loss: 0.0012447743210941553\n",
      "Loss: 0.0011520355474203825\n",
      "Loss: 0.002146346727386117\n",
      "Loss: 0.0023302126210182905\n",
      "Loss: 0.004728300962597132\n",
      "Loss: 0.0011152439983561635\n",
      "Loss: 0.003199153346940875\n",
      "Loss: 0.002001423155888915\n",
      "Loss: 0.0012450864305719733\n",
      "Loss: 0.0023880377411842346\n",
      "Loss: 0.0037046337965875864\n",
      "Loss: 0.001450880547054112\n",
      "Loss: 0.0012738666264340281\n",
      "Loss: 0.0012271153973415494\n",
      "Loss: 0.0023005297407507896\n",
      "Loss: 0.0035328385420143604\n",
      "Loss: 0.003110809950158\n",
      "Loss: 0.0011911846231669188\n",
      "Loss: 0.007793454919010401\n",
      "Loss: 0.002828903729096055\n",
      "Loss: 0.0039116088300943375\n",
      "Loss: 0.002722105709835887\n",
      "Loss: 0.0013846989022567868\n",
      "Loss: 0.0016107138944789767\n",
      "Loss: 0.001530352747067809\n",
      "Loss: 0.00231956853531301\n",
      "Loss: 0.0015445944154635072\n",
      "Loss: 0.002459075069054961\n",
      "Epoch: 37\n",
      "Loss: 0.0020513900090008974\n",
      "Loss: 0.0011846969136968255\n",
      "Loss: 0.00158357759937644\n",
      "Loss: 0.0012290790909901261\n",
      "Loss: 0.0015606024535372853\n",
      "Loss: 0.007461766246706247\n",
      "Loss: 0.0011254396522417665\n",
      "Loss: 0.0032144200522452593\n",
      "Loss: 0.001693498925305903\n",
      "Loss: 0.0011461733374744654\n",
      "Loss: 0.0026426345575600863\n",
      "Loss: 0.0010425406508147717\n",
      "Loss: 0.0010926778195425868\n",
      "Loss: 0.0019087169785052538\n",
      "Loss: 0.0025543454103171825\n",
      "Loss: 0.0013591240858659148\n",
      "Loss: 0.002778824418783188\n",
      "Loss: 0.0015759686939418316\n",
      "Loss: 0.0055488296784460545\n",
      "Loss: 0.0036618602462112904\n",
      "Loss: 0.0013568759895861149\n",
      "Loss: 0.0015105053316801786\n",
      "Loss: 0.0010810215026140213\n",
      "Loss: 0.0013496842002496123\n",
      "Loss: 0.007333049550652504\n",
      "Loss: 0.0010168921435251832\n",
      "Loss: 0.0015640503261238337\n",
      "Loss: 0.007028109859675169\n",
      "Loss: 0.0076749627478420734\n",
      "Loss: 0.0020237802527844906\n",
      "Loss: 0.0018199612386524677\n",
      "Loss: 0.008462734520435333\n",
      "Loss: 0.0013010130496695638\n",
      "Loss: 0.00149094860535115\n",
      "Loss: 0.0015058614080771804\n",
      "Loss: 0.0035557476803660393\n",
      "Loss: 0.0013289726339280605\n",
      "Loss: 0.0036721848882734776\n",
      "Loss: 0.002074648393318057\n",
      "Loss: 0.0023378620389848948\n",
      "Loss: 0.0016741325380280614\n",
      "Loss: 0.0014986004680395126\n",
      "Loss: 0.0010732932714745402\n",
      "Loss: 0.0029374947771430016\n",
      "Loss: 0.002666817046701908\n",
      "Loss: 0.0012432456715032458\n",
      "Loss: 0.0012415560195222497\n",
      "Loss: 0.00193199107889086\n",
      "Loss: 0.0013632866321131587\n",
      "Loss: 0.0014815408503636718\n",
      "Loss: 0.0017464247066527605\n",
      "Epoch: 38\n",
      "Loss: 0.0032330763060599566\n",
      "Loss: 0.001213214360177517\n",
      "Loss: 0.0010484157828614116\n",
      "Loss: 0.004195648245513439\n",
      "Loss: 0.0013980022631585598\n",
      "Loss: 0.002616273006424308\n",
      "Loss: 0.0013271294301375747\n",
      "Loss: 0.0018419005209580064\n",
      "Loss: 0.0009282467653974891\n",
      "Loss: 0.00172957475297153\n",
      "Loss: 0.0011026946594938636\n",
      "Loss: 0.0011119060218334198\n",
      "Loss: 0.0039035168010741472\n",
      "Loss: 0.0012369569158181548\n",
      "Loss: 0.0010520718060433865\n",
      "Loss: 0.0010595424100756645\n",
      "Loss: 0.0018177790334448218\n",
      "Loss: 0.0012131868861615658\n",
      "Loss: 0.0010361120803281665\n",
      "Loss: 0.0015095699345692992\n",
      "Loss: 0.0013624575221911073\n",
      "Loss: 0.005758797749876976\n",
      "Loss: 0.0023327143862843513\n",
      "Loss: 0.0015953454421833158\n",
      "Loss: 0.0010068150004372\n",
      "Loss: 0.0017562112770974636\n",
      "Loss: 0.0010688090696930885\n",
      "Loss: 0.001232126378454268\n",
      "Loss: 0.0017016486963257194\n",
      "Loss: 0.003975475672632456\n",
      "Loss: 0.0016514703165739775\n",
      "Loss: 0.0011675748974084854\n",
      "Loss: 0.0011123070726171136\n",
      "Loss: 0.0012672687880694866\n",
      "Loss: 0.0019739132840186357\n",
      "Loss: 0.002771022729575634\n",
      "Loss: 0.0022244586143642664\n",
      "Loss: 0.0010262305149808526\n",
      "Loss: 0.0016386108472943306\n",
      "Loss: 0.0009360348340123892\n",
      "Loss: 0.0018327353755012155\n",
      "Loss: 0.001234261435456574\n",
      "Loss: 0.0028548571281135082\n",
      "Loss: 0.001121707959100604\n",
      "Loss: 0.001032703323289752\n",
      "Loss: 0.0014195533003658056\n",
      "Loss: 0.0010703185107558966\n",
      "Loss: 0.001886919722892344\n",
      "Loss: 0.0017675893614068627\n",
      "Loss: 0.004883335437625647\n",
      "Loss: 0.0009666020632721484\n",
      "Epoch: 39\n",
      "Loss: 0.00580057455226779\n",
      "Loss: 0.0009621846838854253\n",
      "Loss: 0.0010256314417347312\n",
      "Loss: 0.0015588459791615605\n",
      "Loss: 0.0027599975001066923\n",
      "Loss: 0.0013706731842830777\n",
      "Loss: 0.0009261928498744965\n",
      "Loss: 0.0012426783796399832\n",
      "Loss: 0.01173422671854496\n",
      "Loss: 0.0013165731215849519\n",
      "Loss: 0.0010042432695627213\n",
      "Loss: 0.0015414621448144317\n",
      "Loss: 0.0009451262303628027\n",
      "Loss: 0.0009396392852067947\n",
      "Loss: 0.0009404686279594898\n",
      "Loss: 0.0014124884037300944\n",
      "Loss: 0.00234996504150331\n",
      "Loss: 0.0014834932517260313\n",
      "Loss: 0.0010007696691900492\n",
      "Loss: 0.0020094322971999645\n",
      "Loss: 0.0020012171007692814\n",
      "Loss: 0.0012709378497675061\n",
      "Loss: 0.0014517134986817837\n",
      "Loss: 0.00113817083183676\n",
      "Loss: 0.0010929255513474345\n",
      "Loss: 0.0009128457750193775\n",
      "Loss: 0.001904197153635323\n",
      "Loss: 0.0009076127898879349\n",
      "Loss: 0.0018149210372939706\n",
      "Loss: 0.0009551114635542035\n",
      "Loss: 0.0011603643652051687\n",
      "Loss: 0.0009725898853503168\n",
      "Loss: 0.0009035010589286685\n",
      "Loss: 0.0011937774252146482\n",
      "Loss: 0.0020451494492590427\n",
      "Loss: 0.0015476178377866745\n",
      "Loss: 0.0012684465618804097\n",
      "Loss: 0.0010287190089002252\n",
      "Loss: 0.0013804554473608732\n",
      "Loss: 0.0012030615471303463\n",
      "Loss: 0.0022364703472703695\n",
      "Loss: 0.0010043802903965116\n",
      "Loss: 0.0013266904279589653\n",
      "Loss: 0.000891243340447545\n",
      "Loss: 0.005941405892372131\n",
      "Loss: 0.00282279378734529\n",
      "Loss: 0.001452697440981865\n",
      "Loss: 0.0023189475759863853\n",
      "Loss: 0.0010643565328791738\n",
      "Loss: 0.0011947944294661283\n",
      "Loss: 0.0009125154465436935\n",
      "Epoch: 40\n",
      "Loss: 0.0009652316803112626\n",
      "Loss: 0.0009276584605686367\n",
      "Loss: 0.0009296389180235565\n",
      "Loss: 0.0009709790465421975\n",
      "Loss: 0.0009014626266434789\n",
      "Loss: 0.0008926512091420591\n",
      "Loss: 0.002031066920608282\n",
      "Loss: 0.0008901535184122622\n",
      "Loss: 0.002200680784881115\n",
      "Loss: 0.013006588444113731\n",
      "Loss: 0.0015308479778468609\n",
      "Loss: 0.0009571529808454216\n",
      "Loss: 0.0011534767691046\n",
      "Loss: 0.0010992157040163875\n",
      "Loss: 0.0009023671736940742\n",
      "Loss: 0.0010014342842623591\n",
      "Loss: 0.001277645118534565\n",
      "Loss: 0.0010107314446941018\n",
      "Loss: 0.000861113949213177\n",
      "Loss: 0.0012316959910094738\n",
      "Loss: 0.0009959848830476403\n",
      "Loss: 0.0013208703603595495\n",
      "Loss: 0.002175387227907777\n",
      "Loss: 0.0012772767804563046\n",
      "Loss: 0.0017932686023414135\n",
      "Loss: 0.0011127771576866508\n",
      "Loss: 0.0009301707032136619\n",
      "Loss: 0.0010790076339617372\n",
      "Loss: 0.0018888988997787237\n",
      "Loss: 0.000925370492041111\n",
      "Loss: 0.0031885411590337753\n",
      "Loss: 0.002890966134145856\n",
      "Loss: 0.0011121752904728055\n",
      "Loss: 0.0009462395682930946\n",
      "Loss: 0.0010416337754577398\n",
      "Loss: 0.0009727700380608439\n",
      "Loss: 0.0008179803262464702\n",
      "Loss: 0.0014872679021209478\n",
      "Loss: 0.0009858140256255865\n",
      "Loss: 0.0012594230938702822\n",
      "Loss: 0.0009393797954544425\n",
      "Loss: 0.0009711432503536344\n",
      "Loss: 0.0011347098043188453\n",
      "Loss: 0.00110944255720824\n",
      "Loss: 0.0008187696221284568\n",
      "Loss: 0.0010311370715498924\n",
      "Loss: 0.001017833943478763\n",
      "Loss: 0.001118963467888534\n",
      "Loss: 0.0019317236728966236\n",
      "Loss: 0.0012730646412819624\n",
      "Loss: 0.00093049620045349\n",
      "Epoch: 41\n",
      "Loss: 0.0013469962868839502\n",
      "Loss: 0.0009373229695484042\n",
      "Loss: 0.001015926944091916\n",
      "Loss: 0.0010547463316470385\n",
      "Loss: 0.0014303232310339808\n",
      "Loss: 0.0008452212787233293\n",
      "Loss: 0.000955893425270915\n",
      "Loss: 0.0016165042761713266\n",
      "Loss: 0.0008646960486657917\n",
      "Loss: 0.0009735802887007594\n",
      "Loss: 0.0013571475865319371\n",
      "Loss: 0.00081584946019575\n",
      "Loss: 0.0010842692572623491\n",
      "Loss: 0.0008233781554736197\n",
      "Loss: 0.0010623630369082093\n",
      "Loss: 0.003021295415237546\n",
      "Loss: 0.0008371759322471917\n",
      "Loss: 0.0009423349401913583\n",
      "Loss: 0.001274566981010139\n",
      "Loss: 0.0008735478040762246\n",
      "Loss: 0.0008927526068873703\n",
      "Loss: 0.0015541494358330965\n",
      "Loss: 0.0007928510894998908\n",
      "Loss: 0.0008931683842092752\n",
      "Loss: 0.0008501287084072828\n",
      "Loss: 0.0013837756123393774\n",
      "Loss: 0.0018249661661684513\n",
      "Loss: 0.0010300764115527272\n",
      "Loss: 0.0010940475622192025\n",
      "Loss: 0.0007651136838831007\n",
      "Loss: 0.0009866971522569656\n",
      "Loss: 0.0011004082625731826\n",
      "Loss: 0.0009639424970373511\n",
      "Loss: 0.002880272688344121\n",
      "Loss: 0.0008129873895086348\n",
      "Loss: 0.0011841491796076298\n",
      "Loss: 0.0008902719127945602\n",
      "Loss: 0.0009300957899540663\n",
      "Loss: 0.0008821821538731456\n",
      "Loss: 0.0008515446679666638\n",
      "Loss: 0.0008315027225762606\n",
      "Loss: 0.0027196980081498623\n",
      "Loss: 0.0008083080756478012\n",
      "Loss: 0.001833236892707646\n",
      "Loss: 0.00107899762224406\n",
      "Loss: 0.0012129303067922592\n",
      "Loss: 0.0009578248718753457\n",
      "Loss: 0.001130828750319779\n",
      "Loss: 0.0027026948519051075\n",
      "Loss: 0.0015592147829011083\n",
      "Loss: 0.0009030175278894603\n",
      "Epoch: 42\n",
      "Loss: 0.0009317864314652979\n",
      "Loss: 0.0018783345585688949\n",
      "Loss: 0.0008385832188650966\n",
      "Loss: 0.0010789857478812337\n",
      "Loss: 0.001566566526889801\n",
      "Loss: 0.0010469191474840045\n",
      "Loss: 0.0011657023569568992\n",
      "Loss: 0.0009199986816383898\n",
      "Loss: 0.0010483141522854567\n",
      "Loss: 0.008467569947242737\n",
      "Loss: 0.0009327144944109023\n",
      "Loss: 0.0008385959081351757\n",
      "Loss: 0.0008628959185443819\n",
      "Loss: 0.0007547696586698294\n",
      "Loss: 0.001187054207548499\n",
      "Loss: 0.0012186638778075576\n",
      "Loss: 0.001242204918526113\n",
      "Loss: 0.0010954823810607195\n",
      "Loss: 0.0009666797704994678\n",
      "Loss: 0.0010618777014315128\n",
      "Loss: 0.0007779548759572208\n",
      "Loss: 0.0008472370682284236\n",
      "Loss: 0.0008208304061554372\n",
      "Loss: 0.000958366203121841\n",
      "Loss: 0.0007675508386455476\n",
      "Loss: 0.0008422885439358652\n",
      "Loss: 0.0015102719189599156\n",
      "Loss: 0.014368985779583454\n",
      "Loss: 0.0007579001830890775\n",
      "Loss: 0.0015010416973382235\n",
      "Loss: 0.0008872160105966032\n",
      "Loss: 0.0009972586994990706\n",
      "Loss: 0.0008358164923265576\n",
      "Loss: 0.0008434267365373671\n",
      "Loss: 0.002240127883851528\n",
      "Loss: 0.0007686984608881176\n",
      "Loss: 0.0008302005007863045\n",
      "Loss: 0.0008171686786226928\n",
      "Loss: 0.0008692924748174846\n",
      "Loss: 0.000730978325009346\n",
      "Loss: 0.0013019178295508027\n",
      "Loss: 0.001622103271074593\n",
      "Loss: 0.0010112868621945381\n",
      "Loss: 0.0010262352880090475\n",
      "Loss: 0.0013553352328017354\n",
      "Loss: 0.001120776403695345\n",
      "Loss: 0.001583858160302043\n",
      "Loss: 0.0009234778117388487\n",
      "Loss: 0.0008205948397517204\n",
      "Loss: 0.0014368371339514852\n",
      "Loss: 0.0007925361860543489\n",
      "Epoch: 43\n",
      "Loss: 0.0007873948197811842\n",
      "Loss: 0.0013850878458470106\n",
      "Loss: 0.0009852285729721189\n",
      "Loss: 0.0008769488777033985\n",
      "Loss: 0.0008054462959989905\n",
      "Loss: 0.0012900646543130279\n",
      "Loss: 0.0008528928156010807\n",
      "Loss: 0.0007791891694068909\n",
      "Loss: 0.0016708251787349582\n",
      "Loss: 0.0014817372430115938\n",
      "Loss: 0.005559069570153952\n",
      "Loss: 0.00126203044783324\n",
      "Loss: 0.0008739924523979425\n",
      "Loss: 0.0009680271032266319\n",
      "Loss: 0.00151667685713619\n",
      "Loss: 0.0008475315407849848\n",
      "Loss: 0.0008536997484043241\n",
      "Loss: 0.0007229721522890031\n",
      "Loss: 0.0007876561721786857\n",
      "Loss: 0.00107251713052392\n",
      "Loss: 0.000852674653287977\n",
      "Loss: 0.0008336680475622416\n",
      "Loss: 0.0007241767016239464\n",
      "Loss: 0.0009638697374612093\n",
      "Loss: 0.0009000799618661404\n",
      "Loss: 0.0007527228444814682\n",
      "Loss: 0.0008808873244561255\n",
      "Loss: 0.000825573515612632\n",
      "Loss: 0.0010467214742675424\n",
      "Loss: 0.0009365287842229009\n",
      "Loss: 0.0007062907097861171\n",
      "Loss: 0.0017235707491636276\n",
      "Loss: 0.001152795390225947\n",
      "Loss: 0.0007772513199597597\n",
      "Loss: 0.0011512237833812833\n",
      "Loss: 0.0019855506252497435\n",
      "Loss: 0.0007588606677018106\n",
      "Loss: 0.000796574167907238\n",
      "Loss: 0.0008764385711401701\n",
      "Loss: 0.0009973663836717606\n",
      "Loss: 0.0011517072562128305\n",
      "Loss: 0.0007637928938493133\n",
      "Loss: 0.0012471087975427508\n",
      "Loss: 0.0007325464976020157\n",
      "Loss: 0.001138478284701705\n",
      "Loss: 0.000890903698746115\n",
      "Loss: 0.0017798251938074827\n",
      "Loss: 0.0012055484112352133\n",
      "Loss: 0.0010252122301608324\n",
      "Loss: 0.002040334278717637\n",
      "Loss: 0.0007779374718666077\n",
      "Epoch: 44\n",
      "Loss: 0.0011207334464415908\n",
      "Loss: 0.0007382860640063882\n",
      "Loss: 0.0008039044332690537\n",
      "Loss: 0.0007407329976558685\n",
      "Loss: 0.0007774888072162867\n",
      "Loss: 0.000744988676160574\n",
      "Loss: 0.0007717416738159955\n",
      "Loss: 0.0014380118809640408\n",
      "Loss: 0.0012482191668823361\n",
      "Loss: 0.0008500884287059307\n",
      "Loss: 0.0007771608652547002\n",
      "Loss: 0.0009806999005377293\n",
      "Loss: 0.0012864135205745697\n",
      "Loss: 0.0007507599657401443\n",
      "Loss: 0.001332378014922142\n",
      "Loss: 0.0007830511895008385\n",
      "Loss: 0.0007997815264388919\n",
      "Loss: 0.001078430563211441\n",
      "Loss: 0.0008720323094166815\n",
      "Loss: 0.0012069729855284095\n",
      "Loss: 0.0008017222280614078\n",
      "Loss: 0.000741657568141818\n",
      "Loss: 0.0007133171893656254\n",
      "Loss: 0.0007151825702749193\n",
      "Loss: 0.0016030526021495461\n",
      "Loss: 0.000731426349375397\n",
      "Loss: 0.0006693992181681097\n",
      "Loss: 0.0007730398210696876\n",
      "Loss: 0.0016402314649894834\n",
      "Loss: 0.0009085768251679838\n",
      "Loss: 0.0010990132577717304\n",
      "Loss: 0.0006909271469339728\n",
      "Loss: 0.0006801376002840698\n",
      "Loss: 0.0008319779299199581\n",
      "Loss: 0.0008194458787329495\n",
      "Loss: 0.002366619650274515\n",
      "Loss: 0.000959243974648416\n",
      "Loss: 0.0012011199723929167\n",
      "Loss: 0.000716419774107635\n",
      "Loss: 0.0008214005501940846\n",
      "Loss: 0.0007690850761719048\n",
      "Loss: 0.0008797742775641382\n",
      "Loss: 0.0012592115672305226\n",
      "Loss: 0.0009759175009094179\n",
      "Loss: 0.012646371498703957\n",
      "Loss: 0.0009859494166448712\n",
      "Loss: 0.0007388970698229969\n",
      "Loss: 0.0008115142700262368\n",
      "Loss: 0.000701917102560401\n",
      "Loss: 0.0013471620623022318\n",
      "Loss: 0.0006534912972711027\n",
      "Epoch: 45\n",
      "Loss: 0.0008718417957425117\n",
      "Loss: 0.0007790267118252814\n",
      "Loss: 0.0007226690649986267\n",
      "Loss: 0.0008133391966111958\n",
      "Loss: 0.0010800173040479422\n",
      "Loss: 0.0007463057991117239\n",
      "Loss: 0.0009462510352022946\n",
      "Loss: 0.0006937232101336122\n",
      "Loss: 0.0007251134375110269\n",
      "Loss: 0.0011846352135762572\n",
      "Loss: 0.0009179203188978136\n",
      "Loss: 0.000704617821611464\n",
      "Loss: 0.0011536782840266824\n",
      "Loss: 0.0009481854503974319\n",
      "Loss: 0.0007108792779035866\n",
      "Loss: 0.00084793905261904\n",
      "Loss: 0.0009514042176306248\n",
      "Loss: 0.0009544428903609514\n",
      "Loss: 0.0008352543227374554\n",
      "Loss: 0.0006641296204179525\n",
      "Loss: 0.0031923449132591486\n",
      "Loss: 0.0007060241769067943\n",
      "Loss: 0.0007937740301713347\n",
      "Loss: 0.0009234133176505566\n",
      "Loss: 0.00524714682251215\n",
      "Loss: 0.0008313266444019973\n",
      "Loss: 0.0009212559671141207\n",
      "Loss: 0.0007081064395606518\n",
      "Loss: 0.0006928718066774309\n",
      "Loss: 0.006688131485134363\n",
      "Loss: 0.0011934537906199694\n",
      "Loss: 0.0009178633918054402\n",
      "Loss: 0.0007232356001622975\n",
      "Loss: 0.0006674005999229848\n",
      "Loss: 0.0006960753235034645\n",
      "Loss: 0.0007959065260365605\n",
      "Loss: 0.0022254898212850094\n",
      "Loss: 0.0007377768051810563\n",
      "Loss: 0.0008756095194257796\n",
      "Loss: 0.0006838574772700667\n",
      "Loss: 0.0009736748761497438\n",
      "Loss: 0.0008795841713435948\n",
      "Loss: 0.0007092225714586675\n",
      "Loss: 0.000914248637855053\n",
      "Loss: 0.000695306749548763\n",
      "Loss: 0.0007966176490299404\n",
      "Loss: 0.0008006380521692336\n",
      "Loss: 0.0014421811792999506\n",
      "Loss: 0.001128964009694755\n",
      "Loss: 0.0009447215707041323\n",
      "Loss: 0.0009270311566069722\n",
      "Epoch: 46\n",
      "Loss: 0.0013232195051386952\n",
      "Loss: 0.0006743581034243107\n",
      "Loss: 0.0006608146359212697\n",
      "Loss: 0.0007619883981533349\n",
      "Loss: 0.0022652687039226294\n",
      "Loss: 0.0006710191373713315\n",
      "Loss: 0.0006733753834851086\n",
      "Loss: 0.0008191227097995579\n",
      "Loss: 0.0007401238544844091\n",
      "Loss: 0.0007648270111531019\n",
      "Loss: 0.0012731814058497548\n",
      "Loss: 0.0007166023133322597\n",
      "Loss: 0.0014238596195355058\n",
      "Loss: 0.0006740252138115466\n",
      "Loss: 0.001435228157788515\n",
      "Loss: 0.0009273117175325751\n",
      "Loss: 0.00092749809846282\n",
      "Loss: 0.0008495486108586192\n",
      "Loss: 0.0009978357702493668\n",
      "Loss: 0.0008265236974693835\n",
      "Loss: 0.017495863139629364\n",
      "Loss: 0.0006775819347240031\n",
      "Loss: 0.0006184906815178692\n",
      "Loss: 0.000665011175442487\n",
      "Loss: 0.0008423160761594772\n",
      "Loss: 0.0008181203156709671\n",
      "Loss: 0.012364402413368225\n",
      "Loss: 0.0007010840927250683\n",
      "Loss: 0.0007421140908263624\n",
      "Loss: 0.000776509812567383\n",
      "Loss: 0.0006757845403626561\n",
      "Loss: 0.0016375695122405887\n",
      "Loss: 0.0006799228140152991\n",
      "Loss: 0.001106944982893765\n",
      "Loss: 0.0013936097966507077\n",
      "Loss: 0.0006898988503962755\n",
      "Loss: 0.0008710329420864582\n",
      "Loss: 0.002070703310891986\n",
      "Loss: 0.0008928911993280053\n",
      "Loss: 0.0007424444775097072\n",
      "Loss: 0.0006757844821549952\n",
      "Loss: 0.0020457999780774117\n",
      "Loss: 0.0007321874145418406\n",
      "Loss: 0.0008679780876263976\n",
      "Loss: 0.0009861650178208947\n",
      "Loss: 0.0010280396090820432\n",
      "Loss: 0.0012155318399891257\n",
      "Loss: 0.0018145169597119093\n",
      "Loss: 0.0013454986037686467\n",
      "Loss: 0.0009276655036956072\n",
      "Loss: 0.000656851683743298\n",
      "Epoch: 47\n",
      "Loss: 0.0009025166509672999\n",
      "Loss: 0.0016135952901095152\n",
      "Loss: 0.0007424945943057537\n",
      "Loss: 0.0012906126212328672\n",
      "Loss: 0.0006754360510967672\n",
      "Loss: 0.000645544147118926\n",
      "Loss: 0.0006509406375698745\n",
      "Loss: 0.00103723572101444\n",
      "Loss: 0.009063382633030415\n",
      "Loss: 0.0006357189849950373\n",
      "Loss: 0.0009895917028188705\n",
      "Loss: 0.0006371971685439348\n",
      "Loss: 0.0010187026346102357\n",
      "Loss: 0.00075655517866835\n",
      "Loss: 0.0009291792521253228\n",
      "Loss: 0.0007696382817812264\n",
      "Loss: 0.002584252506494522\n",
      "Loss: 0.001248269691132009\n",
      "Loss: 0.0008648813818581402\n",
      "Loss: 0.0035467033740133047\n",
      "Loss: 0.0007009861874394119\n",
      "Loss: 0.0008586044423282146\n",
      "Loss: 0.0007179002859629691\n",
      "Loss: 0.00072863808600232\n",
      "Loss: 0.0008212047978304327\n",
      "Loss: 0.0007138484506867826\n",
      "Loss: 0.00060657988069579\n",
      "Loss: 0.0009599510813131928\n",
      "Loss: 0.0014306583907455206\n",
      "Loss: 0.009843614883720875\n",
      "Loss: 0.0007941702497191727\n",
      "Loss: 0.0006737064686603844\n",
      "Loss: 0.001750910421833396\n",
      "Loss: 0.001139685744419694\n",
      "Loss: 0.001051108120009303\n",
      "Loss: 0.0013528043637052178\n",
      "Loss: 0.0010741518344730139\n",
      "Loss: 0.0006847558543086052\n",
      "Loss: 0.0006532559636980295\n",
      "Loss: 0.0006189605919644237\n",
      "Loss: 0.006087790243327618\n",
      "Loss: 0.0009090121602639556\n",
      "Loss: 0.0007099599461071193\n",
      "Loss: 0.0007096958579495549\n",
      "Loss: 0.0007596596260555089\n",
      "Loss: 0.0013628751039505005\n",
      "Loss: 0.0006953987758606672\n",
      "Loss: 0.0013625306310132146\n",
      "Loss: 0.0007148823351599276\n",
      "Loss: 0.0011574695818126202\n",
      "Loss: 0.0007304769242182374\n",
      "Epoch: 48\n",
      "Loss: 0.0007130849990062416\n",
      "Loss: 0.0007596879731863737\n",
      "Loss: 0.0006363196880556643\n",
      "Loss: 0.000662406615447253\n",
      "Loss: 0.0008961199200712144\n",
      "Loss: 0.0007847006781958044\n",
      "Loss: 0.0007422672351822257\n",
      "Loss: 0.004776790272444487\n",
      "Loss: 0.0010742454323917627\n",
      "Loss: 0.0034670692402869463\n",
      "Loss: 0.019310198724269867\n",
      "Loss: 0.0007182384142652154\n",
      "Loss: 0.0008381744264625013\n",
      "Loss: 0.0005775863537564874\n",
      "Loss: 0.0010760166915133595\n",
      "Loss: 0.0012105564819648862\n",
      "Loss: 0.00116330839227885\n",
      "Loss: 0.0007006623200140893\n",
      "Loss: 0.011834751814603806\n",
      "Loss: 0.0012172431452199817\n",
      "Loss: 0.0007331236265599728\n",
      "Loss: 0.0008374683675356209\n",
      "Loss: 0.0006158894393593073\n",
      "Loss: 0.0011043157428503036\n",
      "Loss: 0.0006733982008881867\n",
      "Loss: 0.0007226212183013558\n",
      "Loss: 0.0009149917750619352\n",
      "Loss: 0.004519816022366285\n",
      "Loss: 0.0007881422061473131\n",
      "Loss: 0.0008044945425353944\n",
      "Loss: 0.0007319055730476975\n",
      "Loss: 0.002113206312060356\n",
      "Loss: 0.0006266460404731333\n",
      "Loss: 0.0010858034947887063\n",
      "Loss: 0.000823583803139627\n",
      "Loss: 0.0011842460371553898\n",
      "Loss: 0.0015883302548900247\n",
      "Loss: 0.0010270270286127925\n",
      "Loss: 0.0015109694795683026\n",
      "Loss: 0.001268239226192236\n",
      "Loss: 0.001825472922064364\n",
      "Loss: 0.0017282793996855617\n",
      "Loss: 0.0010461402125656605\n",
      "Loss: 0.000622674182523042\n",
      "Loss: 0.0014700260944664478\n",
      "Loss: 0.0006137199234217405\n",
      "Loss: 0.0007883201469667256\n",
      "Loss: 0.0009497487335465848\n",
      "Loss: 0.0006740142125636339\n",
      "Loss: 0.0007756613194942474\n",
      "Loss: 0.0024515301920473576\n",
      "Epoch: 49\n",
      "Loss: 0.0013596757780760527\n",
      "Loss: 0.0009786644950509071\n",
      "Loss: 0.000714305613655597\n",
      "Loss: 0.0007013455033302307\n",
      "Loss: 0.005314723122864962\n",
      "Loss: 0.0006658093770965934\n",
      "Loss: 0.0007078852504491806\n",
      "Loss: 0.0006981792976148427\n",
      "Loss: 0.0007028005784377456\n",
      "Loss: 0.0006411474314518273\n",
      "Loss: 0.0007897198083810508\n",
      "Loss: 0.0006067322683520615\n",
      "Loss: 0.0006271743914112449\n",
      "Loss: 0.0013795262202620506\n",
      "Loss: 0.0007109257858246565\n",
      "Loss: 0.000780646048951894\n",
      "Loss: 0.0008633748511783779\n",
      "Loss: 0.0009923576144501567\n",
      "Loss: 0.0006581569323316216\n",
      "Loss: 0.0012474010000005364\n",
      "Loss: 0.0006657247431576252\n",
      "Loss: 0.0006283760303631425\n",
      "Loss: 0.014461494982242584\n",
      "Loss: 0.000647792883682996\n",
      "Loss: 0.0023120560217648745\n",
      "Loss: 0.0006132858688943088\n",
      "Loss: 0.0005860337405465543\n",
      "Loss: 0.0010056354803964496\n",
      "Loss: 0.0022828730288892984\n",
      "Loss: 0.0009240289800800383\n",
      "Loss: 0.0005717988824471831\n",
      "Loss: 0.0014416695339605212\n",
      "Loss: 0.0007743127644062042\n",
      "Loss: 0.0009354896028526127\n",
      "Loss: 0.0011130328057333827\n",
      "Loss: 0.0014025489799678326\n",
      "Loss: 0.0006034069810993969\n",
      "Loss: 0.0007474985322915018\n",
      "Loss: 0.0005456626531668007\n",
      "Loss: 0.0005638984148390591\n",
      "Loss: 0.0006566543015651405\n",
      "Loss: 0.0011021775426343083\n",
      "Loss: 0.0005961310816928744\n",
      "Loss: 0.0005723837530240417\n",
      "Loss: 0.0008897290099412203\n",
      "Loss: 0.0006146730156615376\n",
      "Loss: 0.0006219652132131159\n",
      "Loss: 0.014138800092041492\n",
      "Loss: 0.002496936358511448\n",
      "Loss: 0.0016870482359081507\n",
      "Loss: 0.0006372462376020849\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "model.train()\n",
    "\n",
    "nb_epochs = 50\n",
    "for epoch in range(nb_epochs):\n",
    "  print(\"Epoch:\", epoch)\n",
    "  avg_loss = 0\n",
    "  for idx, batch in enumerate(train_dataloader):\n",
    "    input_ids = batch.pop(\"input_ids\").to(device)\n",
    "    pixel_values = batch.pop(\"pixel_values\").to(device)\n",
    "\n",
    "    outputs = model(input_ids=input_ids,\n",
    "                    pixel_values=pixel_values,\n",
    "                    labels=input_ids)\n",
    "    \n",
    "    loss = outputs.loss\n",
    "\n",
    "    avg_loss += loss.item() / len(train_dataloader)\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "  print(\"Loss:\", avg_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bw_anvy_py37",
   "language": "python",
   "name": "bw"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
